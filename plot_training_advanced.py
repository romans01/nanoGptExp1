#!/usr/bin/env python3
"""
–£–ª—É—á—à–µ–Ω–Ω—ã–π —Å–∫—Ä–∏–ø—Ç –¥–ª—è –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏ –ª–æ–≥–æ–≤ –æ–±—É—á–µ–Ω–∏—è nanoGPT
- –°–æ—Ö—Ä–∞–Ω—è–µ—Ç –≥—Ä–∞—Ñ–∏–∫–∏ –≤ –æ—Ç–¥–µ–ª—å–Ω—É—é –ø–∞–ø–∫—É
- –î–æ–±–∞–≤–ª—è–µ—Ç –≤—Ä–µ–º–µ–Ω–Ω—ã–µ –º–µ—Ç–∫–∏ –∫ –∏–º–µ–Ω–∞–º —Ñ–∞–π–ª–æ–≤
- –ó–∞–ø–∏—Å—ã–≤–∞–µ—Ç –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –º–æ–¥–µ–ª–∏ –ø–æ–¥ –≥—Ä–∞—Ñ–∏–∫–∞–º–∏
"""

import re
import os
import matplotlib
matplotlib.use('Agg')  # –ò—Å–ø–æ–ª—å–∑—É–µ–º backend –±–µ–∑ GUI
import matplotlib.pyplot as plt
import numpy as np
from pathlib import Path
from datetime import datetime
import json

def create_plots_directory():
    """–°–æ–∑–¥–∞–µ—Ç –ø–∞–ø–∫—É –¥–ª—è –≥—Ä–∞—Ñ–∏–∫–æ–≤ –µ—Å–ª–∏ –µ—ë –Ω–µ—Ç"""
    plots_dir = Path("training_plots")
    plots_dir.mkdir(exist_ok=True)
    return plots_dir

def get_timestamp():
    """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Ç–µ–∫—É—â—É—é –≤—Ä–µ–º–µ–Ω–Ω—É—é –º–µ—Ç–∫—É –¥–ª—è –∏–º–µ–Ω–∏ —Ñ–∞–π–ª–∞"""
    return datetime.now().strftime("%Y%m%d_%H%M%S")

def parse_training_log(log_file_or_text):
    """–ü–∞—Ä—Å–∏—Ç –ª–æ–≥–∏ –æ–±—É—á–µ–Ω–∏—è –∏ –∏–∑–≤–ª–µ–∫–∞–µ—Ç –º–µ—Ç—Ä–∏–∫–∏"""
    
    if isinstance(log_file_or_text, (str, Path)) and Path(log_file_or_text).exists():
        with open(log_file_or_text, 'r') as f:
            text = f.read()
    else:
        text = str(log_file_or_text)
    
    # –†–µ–≥—É–ª—è—Ä–Ω—ã–µ –≤—ã—Ä–∞–∂–µ–Ω–∏—è –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –º–µ—Ç—Ä–∏–∫
    step_pattern = r'step (\d+): train loss ([\d.]+), val loss ([\d.]+)'
    iter_pattern = r'iter (\d+): loss ([\d.]+), time ([\d.]+)ms, mfu ([\d.-]+)%'
    
    # –ò–∑–≤–ª–µ–∫–∞–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –º–æ–¥–µ–ª–∏
    params_pattern = r'number of parameters: ([\d.]+)([MK]?)'
    vocab_pattern = r'vocab_size[=\s]+(\d+)'
    config_patterns = {
        'n_layer': r'n_layer[=\s]+(\d+)',
        'n_head': r'n_head[=\s]+(\d+)', 
        'n_embd': r'n_embd[=\s]+(\d+)',
        'block_size': r'block_size[=\s]+(\d+)',
        'batch_size': r'batch_size[=\s]+(\d+)',
        'learning_rate': r'learning_rate[=\s]+([\d.e-]+)',
        'dropout': r'dropout[=\s]+([\d.]+)',
        'max_iters': r'max_iters[=\s]+(\d+)',
        'dataset': r'dataset[=\s]+[\'"]?([^\'"\s]+)[\'"]?'
    }
    
    steps, train_losses, val_losses = [], [], []
    iters, losses, times, mfus = [], [], [], []
    
    # –ò–∑–≤–ª–µ–∫–∞–µ–º –¥–∞–Ω–Ω—ã–µ –æ validation
    for match in re.finditer(step_pattern, text):
        steps.append(int(match.group(1)))
        train_losses.append(float(match.group(2)))
        val_losses.append(float(match.group(3)))
    
    # –ò–∑–≤–ª–µ–∫–∞–µ–º –¥–∞–Ω–Ω—ã–µ –æ training iterations
    for match in re.finditer(iter_pattern, text):
        iters.append(int(match.group(1)))
        losses.append(float(match.group(2)))
        times.append(float(match.group(3)))
        mfu_val = float(match.group(4))
        # –ò–≥–Ω–æ—Ä–∏—Ä—É–µ–º –æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω—ã–µ MFU (–Ω–∞—á–∞–ª—å–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è)
        mfus.append(max(0, mfu_val))
    
    # –ò–∑–≤–ª–µ–∫–∞–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –º–æ–¥–µ–ª–∏
    model_info = {}
    
    # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
    params_match = re.search(params_pattern, text)
    if params_match:
        num = float(params_match.group(1))
        unit = params_match.group(2)
        if unit == 'M':
            num *= 1e6
        elif unit == 'K':
            num *= 1e3
        model_info['parameters'] = f"{num/1e6:.2f}M"
    
    # –†–∞–∑–º–µ—Ä —Å–ª–æ–≤–∞—Ä—è
    vocab_match = re.search(vocab_pattern, text)
    if vocab_match:
        model_info['vocab_size'] = int(vocab_match.group(1))
    
    # –î—Ä—É–≥–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
    for param, pattern in config_patterns.items():
        match = re.search(pattern, text)
        if match:
            try:
                if param in ['learning_rate', 'dropout']:
                    model_info[param] = float(match.group(1))
                else:
                    model_info[param] = match.group(1)
            except:
                model_info[param] = match.group(1)
    
    return {
        'validation': {'steps': steps, 'train_loss': train_losses, 'val_loss': val_losses},
        'training': {'iters': iters, 'loss': losses, 'time': times, 'mfu': mfus},
        'model_info': model_info
    }

def format_model_info(model_info):
    """–§–æ—Ä–º–∞—Ç–∏—Ä—É–µ—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –º–æ–¥–µ–ª–∏ –¥–ª—è –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è"""
    if not model_info:
        return "Model parameters: Not available"
    
    lines = []
    
    # –û—Å–Ω–æ–≤–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
    if 'parameters' in model_info:
        lines.append(f"Parameters: {model_info['parameters']}")
    if 'vocab_size' in model_info:
        lines.append(f"Vocab size: {model_info['vocab_size']:,}")
    if 'dataset' in model_info:
        lines.append(f"Dataset: {model_info['dataset']}")
    
    # –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞
    arch_params = []
    for param in ['n_layer', 'n_head', 'n_embd', 'block_size']:
        if param in model_info:
            arch_params.append(f"{param}={model_info[param]}")
    if arch_params:
        lines.append(f"Architecture: {', '.join(arch_params)}")
    
    # –û–±—É—á–µ–Ω–∏–µ
    train_params = []
    for param in ['batch_size', 'learning_rate', 'dropout', 'max_iters']:
        if param in model_info:
            if param == 'learning_rate':
                train_params.append(f"lr={model_info[param]}")
            else:
                train_params.append(f"{param}={model_info[param]}")
    if train_params:
        lines.append(f"Training: {', '.join(train_params)}")
    
    return '\n'.join(lines)

def plot_training_metrics(data, save_path=None, title_suffix=""):
    """–°—Ç—Ä–æ–∏—Ç –≥—Ä–∞—Ñ–∏–∫–∏ –º–µ—Ç—Ä–∏–∫ –æ–±—É—á–µ–Ω–∏—è —Å –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏ –º–æ–¥–µ–ª–∏"""
    
    if save_path is None:
        plots_dir = create_plots_directory()
        timestamp = get_timestamp()
        save_path = plots_dir / f"training_metrics_{timestamp}.png"
    
    fig, axes = plt.subplots(2, 2, figsize=(16, 12))
    
    # –ó–∞–≥–æ–ª–æ–≤–æ–∫ —Å –≤—Ä–µ–º–µ–Ω–Ω–æ–π –º–µ—Ç–∫–æ–π
    main_title = f'nanoGPT Training Metrics{title_suffix}'
    if title_suffix:
        main_title += f' - {datetime.now().strftime("%Y-%m-%d %H:%M:%S")}'
    fig.suptitle(main_title, fontsize=16, fontweight='bold')
    
    # –ì—Ä–∞—Ñ–∏–∫ 1: Training –∏ Validation Loss
    if data['validation']['steps']:
        axes[0, 0].plot(data['validation']['steps'], data['validation']['train_loss'], 
                       label='Train Loss', color='blue', marker='o', markersize=4, linewidth=2)
        axes[0, 0].plot(data['validation']['steps'], data['validation']['val_loss'], 
                       label='Validation Loss', color='red', marker='s', markersize=4, linewidth=2)
        axes[0, 0].set_xlabel('Step')
        axes[0, 0].set_ylabel('Loss')
        axes[0, 0].set_title('Train vs Validation Loss')
        axes[0, 0].legend()
        axes[0, 0].grid(True, alpha=0.3)
        axes[0, 0].set_yscale('log')  # –õ–æ–≥–∞—Ä–∏—Ñ–º–∏—á–µ—Å–∫–∞—è —à–∫–∞–ª–∞ –¥–ª—è –ª—É—á—à–µ–π –≤–∏–¥–∏–º–æ—Å—Ç–∏
    
    # –ì—Ä–∞—Ñ–∏–∫ 2: Training Loss (–¥–µ—Ç–∞–ª—å–Ω—ã–π)
    if data['training']['iters']:
        axes[0, 1].plot(data['training']['iters'], data['training']['loss'], 
                       color='green', alpha=0.7, linewidth=1)
        # –î–æ–±–∞–≤–ª—è–µ–º —Å–∫–æ–ª—å–∑—è—â–µ–µ —Å—Ä–µ–¥–Ω–µ–µ
        if len(data['training']['loss']) > 10:
            window = min(50, len(data['training']['loss']) // 10)
            smoothed = np.convolve(data['training']['loss'], np.ones(window)/window, mode='valid')
            smoothed_iters = data['training']['iters'][window-1:]
            axes[0, 1].plot(smoothed_iters, smoothed, color='darkgreen', linewidth=2, 
                           label=f'Smoothed (window={window})')
            axes[0, 1].legend()
        
        axes[0, 1].set_xlabel('Iteration')
        axes[0, 1].set_ylabel('Loss')
        axes[0, 1].set_title('Training Loss (Detailed)')
        axes[0, 1].grid(True, alpha=0.3)
        axes[0, 1].set_yscale('log')
    
    # –ì—Ä–∞—Ñ–∏–∫ 3: Training Time per Iteration
    if data['training']['time']:
        axes[1, 0].plot(data['training']['iters'], data['training']['time'], 
                       color='orange', alpha=0.6, linewidth=1)
        # –°–∫–æ–ª—å–∑—è—â–µ–µ —Å—Ä–µ–¥–Ω–µ–µ –¥–ª—è –≤—Ä–µ–º–µ–Ω–∏
        if len(data['training']['time']) > 10:
            window = min(50, len(data['training']['time']) // 10)
            smoothed_time = np.convolve(data['training']['time'], np.ones(window)/window, mode='valid')
            smoothed_iters = data['training']['iters'][window-1:]
            axes[1, 0].plot(smoothed_iters, smoothed_time, color='darkorange', linewidth=2,
                           label=f'Smoothed (window={window})')
            axes[1, 0].legend()
        
        axes[1, 0].set_xlabel('Iteration')
        axes[1, 0].set_ylabel('Time (ms)')
        axes[1, 0].set_title('Training Time per Iteration')
        axes[1, 0].grid(True, alpha=0.3)
    
    # –ì—Ä–∞—Ñ–∏–∫ 4: Model FLOPs Utilization (MFU)
    if data['training']['mfu']:
        # –§–∏–ª—å—Ç—Ä—É–µ–º –ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è MFU
        valid_mfu = [(i, m) for i, m in zip(data['training']['iters'], data['training']['mfu']) if m > 0]
        if valid_mfu:
            valid_iters, valid_mfu_vals = zip(*valid_mfu)
            axes[1, 1].plot(valid_iters, valid_mfu_vals, 
                           color='purple', alpha=0.7, linewidth=1)
            
            # –°–∫–æ–ª—å–∑—è—â–µ–µ —Å—Ä–µ–¥–Ω–µ–µ –¥–ª—è MFU
            if len(valid_mfu_vals) > 10:
                window = min(50, len(valid_mfu_vals) // 10)
                smoothed_mfu = np.convolve(valid_mfu_vals, np.ones(window)/window, mode='valid')
                smoothed_iters = valid_iters[window-1:]
                axes[1, 1].plot(smoothed_iters, smoothed_mfu, color='darkviolet', linewidth=2,
                               label=f'Smoothed (window={window})')
                axes[1, 1].legend()
        
        axes[1, 1].set_xlabel('Iteration')
        axes[1, 1].set_ylabel('MFU (%)')
        axes[1, 1].set_title('Model FLOPs Utilization')
        axes[1, 1].grid(True, alpha=0.3)
    
    # –î–æ–±–∞–≤–ª—è–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –º–æ–¥–µ–ª–∏ –≤–Ω–∏–∑—É
    model_info_text = format_model_info(data.get('model_info', {}))
    fig.text(0.02, 0.02, model_info_text, fontsize=10, fontfamily='monospace',
             bbox=dict(boxstyle="round,pad=0.5", facecolor="lightgray", alpha=0.8))
    
    plt.tight_layout()
    plt.subplots_adjust(bottom=0.15)  # –û—Å—Ç–∞–≤–ª—è–µ–º –º–µ—Å—Ç–æ –¥–ª—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ –º–æ–¥–µ–ª–∏
    plt.savefig(save_path, dpi=300, bbox_inches='tight')
    print(f"üìä –ì—Ä–∞—Ñ–∏–∫–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤: {save_path}")
    plt.close()
    
    return save_path

def create_summary_report(data, save_path=None):
    """–°–æ–∑–¥–∞–µ—Ç —Ç–µ–∫—Å—Ç–æ–≤—ã–π –æ—Ç—á–µ—Ç —Å –º–µ—Ç—Ä–∏–∫–∞–º–∏"""
    
    if save_path is None:
        plots_dir = create_plots_directory()
        timestamp = get_timestamp()
        save_path = plots_dir / f"training_summary_{timestamp}.txt"
    
    with open(save_path, 'w', encoding='utf-8') as f:
        f.write("=" * 60 + "\n")
        f.write("–û–¢–ß–ï–¢ –ü–û –û–ë–£–ß–ï–ù–ò–Æ –ú–û–î–ï–õ–ò nanoGPT\n")
        f.write("=" * 60 + "\n")
        f.write(f"–°–æ–∑–¥–∞–Ω: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n")
        
        # –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –º–æ–¥–µ–ª–∏
        f.write("–ü–ê–†–ê–ú–ï–¢–†–´ –ú–û–î–ï–õ–ò:\n")
        f.write("-" * 20 + "\n")
        model_info = data.get('model_info', {})
        for key, value in model_info.items():
            f.write(f"{key}: {value}\n")
        f.write("\n")
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –æ–±—É—á–µ–Ω–∏—è
        if data['training']['iters']:
            f.write("–°–¢–ê–¢–ò–°–¢–ò–ö–ê –û–ë–£–ß–ï–ù–ò–Ø:\n")
            f.write("-" * 20 + "\n")
            f.write(f"–û–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∏—Ç–µ—Ä–∞—Ü–∏–π: {max(data['training']['iters'])}\n")
            f.write(f"–ù–∞—á–∞–ª—å–Ω—ã–π training loss: {data['training']['loss'][0]:.4f}\n")
            f.write(f"–§–∏–Ω–∞–ª—å–Ω—ã–π training loss: {data['training']['loss'][-1]:.4f}\n")
            f.write(f"–£–ª—É—á—à–µ–Ω–∏–µ: {data['training']['loss'][0] / data['training']['loss'][-1]:.1f}x\n")
            
            if data['training']['time']:
                avg_time = np.mean(data['training']['time'])
                f.write(f"–°—Ä–µ–¥–Ω–µ–µ –≤—Ä–µ–º—è –Ω–∞ –∏—Ç–µ—Ä–∞—Ü–∏—é: {avg_time:.1f} –º—Å\n")
            
            if data['training']['mfu']:
                valid_mfu = [m for m in data['training']['mfu'] if m > 0]
                if valid_mfu:
                    avg_mfu = np.mean(valid_mfu)
                    f.write(f"–°—Ä–µ–¥–Ω—è—è —É—Ç–∏–ª–∏–∑–∞—Ü–∏—è GPU (MFU): {avg_mfu:.1f}%\n")
            f.write("\n")
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–∏
        if data['validation']['steps']:
            f.write("–°–¢–ê–¢–ò–°–¢–ò–ö–ê –í–ê–õ–ò–î–ê–¶–ò–ò:\n")
            f.write("-" * 20 + "\n")
            f.write(f"–ù–∞—á–∞–ª—å–Ω—ã–π train loss: {data['validation']['train_loss'][0]:.4f}\n")
            f.write(f"–§–∏–Ω–∞–ª—å–Ω—ã–π train loss: {data['validation']['train_loss'][-1]:.4f}\n")
            f.write(f"–ù–∞—á–∞–ª—å–Ω—ã–π val loss: {data['validation']['val_loss'][0]:.4f}\n")
            f.write(f"–§–∏–Ω–∞–ª—å–Ω—ã–π val loss: {data['validation']['val_loss'][-1]:.4f}\n")
            
            # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏–µ
            final_train = data['validation']['train_loss'][-1]
            final_val = data['validation']['val_loss'][-1]
            overfitting = final_val / final_train
            f.write(f"–°–æ–æ—Ç–Ω–æ—à–µ–Ω–∏–µ val/train loss: {overfitting:.2f}")
            if overfitting > 2.0:
                f.write(" (–≤–æ–∑–º–æ–∂–Ω–æ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏–µ)")
            f.write("\n\n")
    
    print(f"üìÑ –û—Ç—á–µ—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω –≤: {save_path}")
    return save_path

def analyze_training_log(log_file):
    """–ü–æ–ª–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –ª–æ–≥–∞ –æ–±—É—á–µ–Ω–∏—è —Å —Å–æ–∑–¥–∞–Ω–∏–µ–º –≥—Ä–∞—Ñ–∏–∫–æ–≤ –∏ –æ—Ç—á–µ—Ç–∞"""
    
    print(f"üîç –ê–Ω–∞–ª–∏–∑–∏—Ä—É—é –ª–æ–≥: {log_file}")
    data = parse_training_log(log_file)
    
    if not data['training']['iters'] and not data['validation']['steps']:
        print("‚ùå –ù–µ –Ω–∞–π–¥–µ–Ω–æ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞")
        return
    
    plots_dir = create_plots_directory()
    timestamp = get_timestamp()
    
    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Å—É—Ñ—Ñ–∏–∫—Å –¥–ª—è –∑–∞–≥–æ–ª–æ–≤–∫–∞
    model_info = data.get('model_info', {})
    title_suffix = ""
    if 'dataset' in model_info:
        title_suffix = f" ({model_info['dataset']})"
    
    # –°–æ–∑–¥–∞–µ–º –≥—Ä–∞—Ñ–∏–∫–∏
    plot_path = plot_training_metrics(data, 
                                    plots_dir / f"training_metrics_{timestamp}.png",
                                    title_suffix)
    
    # –°–æ–∑–¥–∞–µ–º –æ—Ç—á–µ—Ç
    report_path = create_summary_report(data, 
                                      plots_dir / f"training_summary_{timestamp}.txt")
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –¥–∞–Ω–Ω—ã–µ –≤ JSON –¥–ª—è –¥–∞–ª—å–Ω–µ–π—à–µ–≥–æ –∞–Ω–∞–ª–∏–∑–∞
    json_path = plots_dir / f"training_data_{timestamp}.json"
    with open(json_path, 'w') as f:
        # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º numpy arrays –≤ —Å–ø–∏—Å–∫–∏ –¥–ª—è JSON
        json_data = {
            'validation': {
                'steps': data['validation']['steps'],
                'train_loss': data['validation']['train_loss'],
                'val_loss': data['validation']['val_loss']
            },
            'training': {
                'iters': data['training']['iters'],
                'loss': data['training']['loss'],
                'time': data['training']['time'],
                'mfu': data['training']['mfu']
            },
            'model_info': data['model_info']
        }
        json.dump(json_data, f, indent=2)
    
    print(f"üíæ –î–∞–Ω–Ω—ã–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤: {json_path}")
    print(f"‚úÖ –ê–Ω–∞–ª–∏–∑ –∑–∞–≤–µ—Ä—à–µ–Ω! –§–∞–π–ª—ã –≤ –ø–∞–ø–∫–µ: {plots_dir}")

if __name__ == "__main__":
    import sys
    
    if len(sys.argv) > 1:
        log_file = sys.argv[1]
        if Path(log_file).exists():
            analyze_training_log(log_file)
        else:
            print(f"‚ùå –§–∞–π–ª {log_file} –Ω–µ –Ω–∞–π–¥–µ–Ω")
    else:
        # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —Ç–µ–∫—É—â–∏–π –ª–æ–≥ –µ—Å–ª–∏ –æ–Ω –µ—Å—Ç—å
        if Path("training.log").exists():
            analyze_training_log("training.log")
        else:
            print("‚ùå –ù–µ –Ω–∞–π–¥–µ–Ω —Ñ–∞–π–ª training.log")
            print("–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ: python plot_training_advanced.py [–ø—É—Ç—å_–∫_–ª–æ–≥—É]")
