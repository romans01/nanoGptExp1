#!/usr/bin/env python3
"""
–£–ª—É—á—à–µ–Ω–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏—è –≥—Ä–∞—Ñ–∏–∫–æ–≤ –æ–±—É—á–µ–Ω–∏—è —Å:
- –ß–µ—Ç–∫–∏–º–∏ –ø–æ–¥–ø–∏—Å—è–º–∏ –º–æ–¥–µ–ª–µ–π
- –°—Ä–∞–≤–Ω–µ–Ω–∏–µ–º –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –º–æ–¥–µ–ª–µ–π –Ω–∞ –æ–¥–Ω–æ–º –≥—Ä–∞—Ñ–∏–∫–µ
- –ü–æ–Ω—è—Ç–Ω—ã–º–∏ –ø–æ–¥–ø–∏—Å—è–º–∏ –æ—Å–µ–π (–±–µ–∑ –Ω–∞—É—á–Ω–æ–π –Ω–æ—Ç–∞—Ü–∏–∏)
- –õ—É—á—à–∏–º —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ–º
"""

import re
import matplotlib
matplotlib.use('Agg')
import matplotlib.pyplot as plt
import matplotlib.ticker as ticker
import numpy as np
from pathlib import Path
import datetime
import json
import os

def get_timestamp():
    """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Ç–µ–∫—É—â—É—é –≤—Ä–µ–º–µ–Ω–Ω—É—é –º–µ—Ç–∫—É"""
    return datetime.datetime.now().strftime("%Y%m%d_%H%M%S")

def create_plots_directory():
    """–°–æ–∑–¥–∞–µ—Ç –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –¥–ª—è –≥—Ä–∞—Ñ–∏–∫–æ–≤ –µ—Å–ª–∏ –µ—ë –Ω–µ—Ç"""
    plots_dir = Path("training_plots")
    plots_dir.mkdir(exist_ok=True)
    return plots_dir

def parse_training_log(log_file_or_text):
    """–ü–∞—Ä—Å–∏—Ç –ª–æ–≥ –æ–±—É—á–µ–Ω–∏—è –∏ –∏–∑–≤–ª–µ–∫–∞–µ—Ç –º–µ—Ç—Ä–∏–∫–∏"""
    
    if isinstance(log_file_or_text, (str, Path)) and Path(log_file_or_text).exists():
        with open(log_file_or_text, 'r') as f:
            text = f.read()
    else:
        text = str(log_file_or_text)
    
    data = {
        'training': {'iters': [], 'loss': [], 'time': [], 'mfu': []},
        'validation': {'steps': [], 'train_loss': [], 'val_loss': []},
        'model_info': {},
        'config_info': {}
    }
    
    # –ò–∑–≤–ª–µ–∫–∞–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –º–æ–¥–µ–ª–∏
    model_patterns = {
        'parameters': r'number of parameters: ([\d.]+)M',
        'n_layer': r'n_layer = (\d+)',
        'n_head': r'n_head = (\d+)', 
        'n_embd': r'n_embd = (\d+)',
        'batch_size': r'batch_size = (\d+)',
        'learning_rate': r'learning_rate = ([\d.e-]+)',
        'max_iters': r'max_iters = (\d+)',
        'dataset': r'dataset = (\w+)',
        'block_size': r'block_size = (\d+)',
        'dropout': r'dropout = ([\d.]+)'
    }
    
    for key, pattern in model_patterns.items():
        match = re.search(pattern, text)
        if match:
            try:
                if key in ['n_layer', 'n_head', 'n_embd', 'batch_size', 'max_iters', 'block_size']:
                    data['model_info'][key] = int(match.group(1))
                elif key in ['learning_rate', 'dropout']:
                    data['model_info'][key] = float(match.group(1))
                else:
                    data['model_info'][key] = match.group(1)
            except ValueError:
                data['model_info'][key] = match.group(1)
    
    # –ò–∑–≤–ª–µ–∫–∞–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ —Ü–µ–Ω—Ç—Ä–∏—Ä–æ–≤–∞–Ω–∏–∏
    centering_patterns = {
        'use_centered_attention': r'use_centered_attention = (True|False)',
        'center_qk': r'center_qk = (True|False)',
        'center_final_output': r'center_final_output = (True|False)',
        'center_block_output': r'center_block_output = (True|False)',
        'centering_mode': r'centering_mode = [\'"](\w+)[\'"]'
    }
    
    for key, pattern in centering_patterns.items():
        match = re.search(pattern, text)
        if match:
            if key in ['use_centered_attention', 'center_qk', 'center_final_output', 'center_block_output']:
                data['config_info'][key] = match.group(1) == 'True'
            else:
                data['config_info'][key] = match.group(1)
    
    # –ü–∞—Ä—Å–∏–º –º–µ—Ç—Ä–∏–∫–∏ –æ–±—É—á–µ–Ω–∏—è
    iter_pattern = r'iter (\d+): loss ([\d.]+), time ([\d.]+)ms, mfu ([-\d.]+)%'
    for match in re.finditer(iter_pattern, text):
        data['training']['iters'].append(int(match.group(1)))
        data['training']['loss'].append(float(match.group(2)))
        data['training']['time'].append(float(match.group(3)))
        data['training']['mfu'].append(float(match.group(4)))
    
    # –ü–∞—Ä—Å–∏–º –º–µ—Ç—Ä–∏–∫–∏ –≤–∞–ª–∏–¥–∞—Ü–∏–∏
    step_pattern = r'step (\d+): train loss ([\d.]+), val loss ([\d.]+)'
    for match in re.finditer(step_pattern, text):
        data['validation']['steps'].append(int(match.group(1)))
        data['validation']['train_loss'].append(float(match.group(2)))
        data['validation']['val_loss'].append(float(match.group(3)))
    
    return data

def determine_model_type(data):
    """–û–ø—Ä–µ–¥–µ–ª—è–µ—Ç —Ç–∏–ø –º–æ–¥–µ–ª–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏"""
    config = data.get('config_info', {})
    
    if config.get('use_centered_attention') and config.get('center_qk'):
        if config.get('center_final_output'):
            return "Full Centered (QK + Final)"
        else:
            return "QK Centered"
    elif config.get('center_final_output'):
        return "Final Centered"
    elif config.get('center_block_output'):
        return "Block Centered"
    else:
        return "Baseline"

def format_model_info(data):
    """–§–æ—Ä–º–∞—Ç–∏—Ä—É–µ—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –º–æ–¥–µ–ª–∏ –¥–ª—è –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è"""
    model_info = data.get('model_info', {})
    config_info = data.get('config_info', {})
    
    lines = []
    
    # –¢–∏–ø –º–æ–¥–µ–ª–∏
    model_type = determine_model_type(data)
    lines.append(f"Model: {model_type}")
    
    # –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞
    arch_params = []
    for param in ['n_layer', 'n_head', 'n_embd']:
        if param in model_info:
            arch_params.append(f"{param.split('_')[1]}={model_info[param]}")
    if arch_params:
        lines.append(f"Architecture: {', '.join(arch_params)}")
    
    # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã
    if 'parameters' in model_info:
        lines.append(f"Parameters: {model_info['parameters']}")
    
    # –û–±—É—á–µ–Ω–∏–µ
    train_params = []
    for param in ['batch_size', 'learning_rate', 'max_iters', 'dataset']:
        if param in model_info:
            if param == 'learning_rate':
                train_params.append(f"lr={model_info[param]}")
            else:
                train_params.append(f"{param}={model_info[param]}")
    if train_params:
        lines.append(f"Training: {', '.join(train_params)}")
    
    return '\n'.join(lines)

def format_axis_labels(ax, axis='y'):
    """–§–æ—Ä–º–∞—Ç–∏—Ä—É–µ—Ç –ø–æ–¥–ø–∏—Å–∏ –æ—Å–µ–π –±–µ–∑ –Ω–∞—É—á–Ω–æ–π –Ω–æ—Ç–∞—Ü–∏–∏"""
    if axis == 'y':
        ax.yaxis.set_major_formatter(ticker.FuncFormatter(lambda x, p: f'{x:.3f}' if x < 1 else f'{x:.1f}'))
    elif axis == 'x':
        ax.xaxis.set_major_formatter(ticker.FuncFormatter(lambda x, p: f'{int(x)}'))

def plot_single_model(data, save_path=None, title_suffix=""):
    """–°—Ç—Ä–æ–∏—Ç –≥—Ä–∞—Ñ–∏–∫–∏ –¥–ª—è –æ–¥–Ω–æ–π –º–æ–¥–µ–ª–∏"""
    
    if save_path is None:
        plots_dir = create_plots_directory()
        timestamp = get_timestamp()
        model_type = determine_model_type(data).replace(' ', '_').lower()
        save_path = plots_dir / f"training_metrics_{model_type}_{timestamp}.png"
    
    fig, axes = plt.subplots(2, 2, figsize=(16, 12))
    
    # –ó–∞–≥–æ–ª–æ–≤–æ–∫ —Å —Ç–∏–ø–æ–º –º–æ–¥–µ–ª–∏
    model_type = determine_model_type(data)
    main_title = f'nanoGPT Training: {model_type}{title_suffix}'
    fig.suptitle(main_title, fontsize=16, fontweight='bold')
    
    # –ì—Ä–∞—Ñ–∏–∫ 1: Training –∏ Validation Loss
    if data['validation']['steps']:
        axes[0, 0].plot(data['validation']['steps'], data['validation']['train_loss'], 
                       label='Train Loss', color='blue', marker='o', markersize=4, linewidth=2)
        axes[0, 0].plot(data['validation']['steps'], data['validation']['val_loss'], 
                       label='Validation Loss', color='red', marker='s', markersize=4, linewidth=2)
        axes[0, 0].set_xlabel('Iteration')
        axes[0, 0].set_ylabel('Loss')
        axes[0, 0].set_title('Train vs Validation Loss')
        axes[0, 0].legend()
        axes[0, 0].grid(True, alpha=0.3)
        format_axis_labels(axes[0, 0], 'y')
        format_axis_labels(axes[0, 0], 'x')
    
    # –ì—Ä–∞—Ñ–∏–∫ 2: Training Loss (–¥–µ—Ç–∞–ª—å–Ω—ã–π)
    if data['training']['iters']:
        axes[0, 1].plot(data['training']['iters'], data['training']['loss'], 
                       color='green', alpha=0.7, linewidth=1, label='Training Loss')
        # –î–æ–±–∞–≤–ª—è–µ–º —Å–∫–æ–ª—å–∑—è—â–µ–µ —Å—Ä–µ–¥–Ω–µ–µ
        if len(data['training']['loss']) > 10:
            window = min(50, len(data['training']['loss']) // 10)
            smoothed = np.convolve(data['training']['loss'], np.ones(window)/window, mode='valid')
            smoothed_iters = data['training']['iters'][window-1:]
            axes[0, 1].plot(smoothed_iters, smoothed, color='darkgreen', linewidth=2, 
                           label=f'Smoothed (window={window})')
        
        axes[0, 1].set_xlabel('Iteration')
        axes[0, 1].set_ylabel('Loss')
        axes[0, 1].set_title('Detailed Training Loss')
        axes[0, 1].legend()
        axes[0, 1].grid(True, alpha=0.3)
        format_axis_labels(axes[0, 1], 'y')
        format_axis_labels(axes[0, 1], 'x')
    
    # –ì—Ä–∞—Ñ–∏–∫ 3: –í—Ä–µ–º—è –Ω–∞ –∏—Ç–µ—Ä–∞—Ü–∏—é
    if data['training']['time']:
        axes[1, 0].plot(data['training']['iters'], data['training']['time'], 
                       color='orange', alpha=0.7, linewidth=1)
        # –°–∫–æ–ª—å–∑—è—â–µ–µ —Å—Ä–µ–¥–Ω–µ–µ –¥–ª—è –≤—Ä–µ–º–µ–Ω–∏
        if len(data['training']['time']) > 10:
            window = min(20, len(data['training']['time']) // 5)
            smoothed_time = np.convolve(data['training']['time'], np.ones(window)/window, mode='valid')
            smoothed_iters = data['training']['iters'][window-1:]
            axes[1, 0].plot(smoothed_iters, smoothed_time, color='darkorange', linewidth=2,
                           label=f'Smoothed (window={window})')
            axes[1, 0].legend()
        
        axes[1, 0].set_xlabel('Iteration')
        axes[1, 0].set_ylabel('Time (ms)')
        axes[1, 0].set_title('Training Time per Iteration')
        axes[1, 0].grid(True, alpha=0.3)
        format_axis_labels(axes[1, 0], 'y')
        format_axis_labels(axes[1, 0], 'x')
    
    # –ì—Ä–∞—Ñ–∏–∫ 4: MFU (Model FLOPs Utilization)
    if data['training']['mfu'] and any(mfu > 0 for mfu in data['training']['mfu']):
        valid_mfu = [(iter_num, mfu) for iter_num, mfu in zip(data['training']['iters'], data['training']['mfu']) if mfu > 0]
        if valid_mfu:
            valid_iters, valid_mfu_vals = zip(*valid_mfu)
            axes[1, 1].plot(valid_iters, valid_mfu_vals, color='purple', alpha=0.7, linewidth=1)
            # –°–∫–æ–ª—å–∑—è—â–µ–µ —Å—Ä–µ–¥–Ω–µ–µ –¥–ª—è MFU
            if len(valid_mfu_vals) > 10:
                window = min(20, len(valid_mfu_vals) // 5)
                smoothed_mfu = np.convolve(valid_mfu_vals, np.ones(window)/window, mode='valid')
                smoothed_iters = valid_iters[window-1:]
                axes[1, 1].plot(smoothed_iters, smoothed_mfu, color='darkviolet', linewidth=2,
                               label=f'Smoothed (window={window})')
                axes[1, 1].legend()
        
        axes[1, 1].set_xlabel('Iteration')
        axes[1, 1].set_ylabel('MFU (%)')
        axes[1, 1].set_title('Model FLOPs Utilization')
        axes[1, 1].grid(True, alpha=0.3)
        format_axis_labels(axes[1, 1], 'y')
        format_axis_labels(axes[1, 1], 'x')
    
    # –î–æ–±–∞–≤–ª—è–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –º–æ–¥–µ–ª–∏
    model_info_text = format_model_info(data)
    fig.text(0.02, 0.02, model_info_text, fontsize=9, verticalalignment='bottom',
             bbox=dict(boxstyle='round,pad=0.5', facecolor='lightblue', alpha=0.8))
    
    plt.tight_layout()
    plt.subplots_adjust(bottom=0.15)  # –ú–µ—Å—Ç–æ –¥–ª—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ –º–æ–¥–µ–ª–∏
    plt.savefig(save_path, dpi=300, bbox_inches='tight')
    plt.close()
    
    return save_path

def plot_comparison(data_list, model_names, save_path=None, title="Model Comparison"):
    """–°—Ä–∞–≤–Ω–∏–≤–∞–µ—Ç –Ω–µ—Å–∫–æ–ª—å–∫–æ –º–æ–¥–µ–ª–µ–π –Ω–∞ –æ–¥–Ω–æ–º –≥—Ä–∞—Ñ–∏–∫–µ"""
    
    if save_path is None:
        plots_dir = create_plots_directory()
        timestamp = get_timestamp()
        save_path = plots_dir / f"comparison_{timestamp}.png"
    
    fig, axes = plt.subplots(2, 2, figsize=(16, 12))
    
    # –ó–∞–≥–æ–ª–æ–≤–æ–∫
    fig.suptitle(title, fontsize=16, fontweight='bold')
    
    # –¶–≤–µ—Ç–∞ –¥–ª—è —Ä–∞–∑–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π
    colors = ['blue', 'red', 'green', 'orange', 'purple', 'brown', 'pink', 'gray']
    
    # –ì—Ä–∞—Ñ–∏–∫ 1: Validation Loss —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ
    for i, (data, name) in enumerate(zip(data_list, model_names)):
        if data['validation']['steps']:
            color = colors[i % len(colors)]
            axes[0, 0].plot(data['validation']['steps'], data['validation']['val_loss'], 
                           label=name, color=color, marker='o', markersize=3, linewidth=2)
    
    axes[0, 0].set_xlabel('Iteration')
    axes[0, 0].set_ylabel('Validation Loss')
    axes[0, 0].set_title('Validation Loss Comparison')
    axes[0, 0].legend()
    axes[0, 0].grid(True, alpha=0.3)
    format_axis_labels(axes[0, 0], 'y')
    format_axis_labels(axes[0, 0], 'x')
    
    # –ì—Ä–∞—Ñ–∏–∫ 2: Training Loss —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ (—Å–≥–ª–∞–∂–µ–Ω–Ω—ã–π)
    for i, (data, name) in enumerate(zip(data_list, model_names)):
        if data['training']['iters'] and len(data['training']['loss']) > 10:
            color = colors[i % len(colors)]
            window = min(50, len(data['training']['loss']) // 10)
            smoothed = np.convolve(data['training']['loss'], np.ones(window)/window, mode='valid')
            smoothed_iters = data['training']['iters'][window-1:]
            axes[0, 1].plot(smoothed_iters, smoothed, color=color, linewidth=2, label=name)
    
    axes[0, 1].set_xlabel('Iteration')
    axes[0, 1].set_ylabel('Training Loss (Smoothed)')
    axes[0, 1].set_title('Training Loss Comparison')
    axes[0, 1].legend()
    axes[0, 1].grid(True, alpha=0.3)
    format_axis_labels(axes[0, 1], 'y')
    format_axis_labels(axes[0, 1], 'x')
    
    # –ì—Ä–∞—Ñ–∏–∫ 3: –í—Ä–µ–º—è –æ–±—É—á–µ–Ω–∏—è —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ
    for i, (data, name) in enumerate(zip(data_list, model_names)):
        if data['training']['time'] and len(data['training']['time']) > 10:
            color = colors[i % len(colors)]
            window = min(20, len(data['training']['time']) // 5)
            smoothed_time = np.convolve(data['training']['time'], np.ones(window)/window, mode='valid')
            smoothed_iters = data['training']['iters'][window-1:]
            axes[1, 0].plot(smoothed_iters, smoothed_time, color=color, linewidth=2, label=name)
    
    axes[1, 0].set_xlabel('Iteration')
    axes[1, 0].set_ylabel('Time per Iteration (ms)')
    axes[1, 0].set_title('Training Time Comparison')
    axes[1, 0].legend()
    axes[1, 0].grid(True, alpha=0.3)
    format_axis_labels(axes[1, 0], 'y')
    format_axis_labels(axes[1, 0], 'x')
    
    # –ì—Ä–∞—Ñ–∏–∫ 4: –§–∏–Ω–∞–ª—å–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã (–±–∞—Ä–Ω—ã–π –≥—Ä–∞—Ñ–∏–∫)
    final_losses = []
    final_times = []
    
    for data in data_list:
        if data['validation']['val_loss']:
            final_losses.append(data['validation']['val_loss'][-1])
        else:
            final_losses.append(None)
        
        if data['training']['time']:
            final_times.append(np.mean(data['training']['time'][-50:]))  # –°—Ä–µ–¥–Ω–µ–µ –∑–∞ –ø–æ—Å–ª–µ–¥–Ω–∏–µ 50 –∏—Ç–µ—Ä–∞—Ü–∏–π
        else:
            final_times.append(None)
    
    # –ë–∞—Ä–Ω—ã–π –≥—Ä–∞—Ñ–∏–∫ —Ñ–∏–Ω–∞–ª—å–Ω—ã—Ö validation loss
    valid_losses = [(name, loss) for name, loss in zip(model_names, final_losses) if loss is not None]
    if valid_losses:
        names, losses = zip(*valid_losses)
        bars = axes[1, 1].bar(range(len(names)), losses, color=colors[:len(names)])
        axes[1, 1].set_xlabel('Model')
        axes[1, 1].set_ylabel('Final Validation Loss')
        axes[1, 1].set_title('Final Results Comparison')
        axes[1, 1].set_xticks(range(len(names)))
        axes[1, 1].set_xticklabels(names, rotation=45, ha='right')
        axes[1, 1].grid(True, alpha=0.3, axis='y')
        format_axis_labels(axes[1, 1], 'y')
        
        # –î–æ–±–∞–≤–ª—è–µ–º –∑–Ω–∞—á–µ–Ω–∏—è –Ω–∞ —Å—Ç–æ–ª–±—Ü—ã
        for bar, loss in zip(bars, losses):
            height = bar.get_height()
            axes[1, 1].text(bar.get_x() + bar.get_width()/2., height + height*0.01,
                            f'{loss:.3f}', ha='center', va='bottom', fontweight='bold')
    
    plt.tight_layout()
    plt.savefig(save_path, dpi=300, bbox_inches='tight')
    plt.close()
    
    return save_path

def compare_best_models():
    """–°—Ä–∞–≤–Ω–∏–≤–∞–µ—Ç –ª—É—á—à–∏–µ –º–æ–¥–µ–ª–∏ –∏–∑ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–æ–≤"""
    
    # –ò—â–µ–º –ª–æ–≥–∏ –ª—É—á—à–∏—Ö –º–æ–¥–µ–ª–µ–π
    model_dirs = [
        ('out-centering-bpe-baseline_bpe', 'Baseline'),
        ('out-centering-bpe-qk_centered_bpe', 'QK Centered')
    ]
    
    data_list = []
    model_names = []
    
    for model_dir, model_name in model_dirs:
        # –ò—â–µ–º —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏–π –ª–æ–≥ –≤ training_plots
        log_files = list(Path('training_plots').glob('training_data_*.json'))
        
        # –ü—ã—Ç–∞–µ–º—Å—è –Ω–∞–π—Ç–∏ –ª–æ–≥ –¥–ª—è —ç—Ç–æ–π –º–æ–¥–µ–ª–∏
        model_data = None
        for log_file in log_files:
            try:
                with open(log_file, 'r') as f:
                    data = json.load(f)
                    
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É–µ—Ç –ª–∏ —ç—Ç–æ—Ç –ª–æ–≥ –Ω–∞—à–µ–π –º–æ–¥–µ–ª–∏
                if model_name == 'Baseline':
                    if not data.get('config_info', {}).get('use_centered_attention', True):
                        model_data = data
                        break
                elif model_name == 'QK Centered':
                    config = data.get('config_info', {})
                    if (config.get('use_centered_attention') and 
                        config.get('center_qk') and 
                        not config.get('center_final_output')):
                        model_data = data
                        break
            except:
                continue
        
        if model_data:
            data_list.append(model_data)
            model_names.append(model_name)
    
    if len(data_list) >= 2:
        save_path = plot_comparison(data_list, model_names, 
                                   title="Best Models Comparison: Baseline vs QK Centered")
        print(f"üìä –°—Ä–∞–≤–Ω–∏—Ç–µ–ª—å–Ω—ã–π –≥—Ä–∞—Ñ–∏–∫ —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {save_path}")
        return save_path
    else:
        print("‚ùå –ù–µ –Ω–∞–π–¥–µ–Ω–æ –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è")
        return None

def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è"""
    import sys
    
    if len(sys.argv) > 1:
        if sys.argv[1] == 'compare':
            compare_best_models()
        else:
            # –ü–∞—Ä—Å–∏–º —É–∫–∞–∑–∞–Ω–Ω—ã–π –ª–æ–≥ —Ñ–∞–π–ª
            log_file = sys.argv[1]
            if os.path.exists(log_file):
                data = parse_training_log(log_file)
                save_path = plot_single_model(data)
                print(f"üìä –ì—Ä–∞—Ñ–∏–∫ —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {save_path}")
            else:
                print(f"‚ùå –§–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω: {log_file}")
    else:
        # –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é —Å—Ä–∞–≤–Ω–∏–≤–∞–µ–º –ª—É—á—à–∏–µ –º–æ–¥–µ–ª–∏
        compare_best_models()

if __name__ == "__main__":
    main()
