{
  "validation": {
    "steps": [
      0,
      50,
      100,
      150,
      200,
      250,
      300,
      350,
      400,
      450,
      500,
      550,
      600,
      650,
      700,
      750,
      800,
      850,
      900,
      950,
      1000
    ],
    "train_loss": [
      10.826,
      10.637,
      10.2874,
      9.8929,
      9.4326,
      8.9546,
      8.8442,
      8.5376,
      8.1987,
      7.9064,
      7.6609,
      7.4725,
      7.3123,
      7.2368,
      7.1281,
      7.0077,
      6.9652,
      6.9117,
      6.8878,
      6.8634,
      6.8398
    ],
    "val_loss": [
      10.8259,
      10.6429,
      10.3123,
      9.9511,
      9.5164,
      9.0204,
      8.9278,
      8.6213,
      8.2939,
      8.0009,
      7.7826,
      7.5775,
      7.4426,
      7.3691,
      7.2215,
      7.1438,
      7.0751,
      7.0507,
      7.0232,
      6.9952,
      6.9694
    ]
  },
  "training": {
    "iters": [
      0,
      10,
      20,
      30,
      40,
      50,
      60,
      70,
      80,
      90,
      100,
      110,
      120,
      130,
      140,
      150,
      160,
      170,
      180,
      190,
      200,
      210,
      220,
      230,
      240,
      250,
      260,
      270,
      280,
      290,
      300,
      310,
      320,
      330,
      340,
      350,
      360,
      370,
      380,
      390,
      400,
      410,
      420,
      430,
      440,
      450,
      460,
      470,
      480,
      490,
      500,
      510,
      520,
      530,
      540,
      550,
      560,
      570,
      580,
      590,
      600,
      610,
      620,
      630,
      640,
      650,
      660,
      670,
      680,
      690,
      700,
      710,
      720,
      730,
      740,
      750,
      760,
      770,
      780,
      790,
      800,
      810,
      820,
      830,
      840,
      850,
      860,
      870,
      880,
      890,
      900,
      910,
      920,
      930,
      940,
      950,
      960,
      970,
      980,
      990,
      1000
    ],
    "loss": [
      10.826,
      10.7675,
      10.7381,
      10.715,
      10.682,
      10.6338,
      10.5932,
      10.5446,
      10.4581,
      10.3901,
      10.2962,
      10.2003,
      10.1032,
      10.0197,
      9.9015,
      10.2144,
      9.8296,
      9.7346,
      9.6149,
      9.5409,
      9.4486,
      9.3513,
      9.3031,
      9.2197,
      9.5179,
      9.4271,
      9.3667,
      9.2977,
      9.2348,
      9.0707,
      9.2447,
      9.1314,
      9.0464,
      8.9847,
      8.9031,
      8.7944,
      8.7185,
      8.6434,
      8.5743,
      8.4698,
      8.3859,
      8.3369,
      8.2296,
      8.2407,
      8.1387,
      7.9784,
      8.0521,
      7.9643,
      7.901,
      7.8819,
      7.7196,
      7.7648,
      7.7026,
      7.5706,
      7.6124,
      7.5517,
      7.4506,
      7.4398,
      7.4814,
      7.283,
      7.2304,
      7.3118,
      7.2317,
      7.2832,
      7.1825,
      7.3037,
      7.284,
      7.0733,
      7.1787,
      7.1142,
      7.1784,
      7.0912,
      7.0637,
      6.9762,
      7.1382,
      7.0555,
      7.0628,
      6.9041,
      7.0149,
      6.9497,
      6.9306,
      7.0407,
      6.9402,
      6.9317,
      6.9407,
      6.8024,
      6.9172,
      6.8851,
      6.9566,
      6.9667,
      6.7546,
      6.8605,
      6.912,
      6.7829,
      6.9345,
      6.8401,
      6.8522,
      6.9094,
      6.8325,
      6.8664,
      6.8871
    ],
    "time": [
      5306.17,
      252.21,
      254.87,
      256.52,
      254.23,
      3445.86,
      257.42,
      253.69,
      253.6,
      255.78,
      3220.12,
      252.04,
      254.28,
      254.67,
      253.09,
      2949.26,
      253.36,
      253.28,
      253.12,
      250.61,
      3256.5,
      249.68,
      251.53,
      253.9,
      252.23,
      3223.55,
      252.92,
      251.33,
      250.58,
      252.98,
      3632.69,
      253.9,
      253.5,
      252.09,
      253.4,
      3297.41,
      250.67,
      252.94,
      252.54,
      252.32,
      3554.84,
      250.02,
      250.69,
      254.4,
      253.22,
      3555.71,
      253.78,
      254.45,
      252.84,
      253.62,
      3549.69,
      250.26,
      254.7,
      253.1,
      253.0,
      3242.4,
      253.75,
      253.48,
      252.3,
      251.25,
      3546.88,
      251.78,
      254.33,
      250.73,
      250.72,
      3564.47,
      253.33,
      251.17,
      254.81,
      252.64,
      3200.92,
      252.61,
      255.39,
      251.79,
      252.14,
      3263.88,
      250.42,
      251.8,
      251.66,
      253.94,
      3561.04,
      253.69,
      253.06,
      252.41,
      254.04,
      3363.77,
      253.38,
      251.72,
      250.61,
      251.79,
      3347.64,
      252.43,
      254.19,
      256.45,
      253.73,
      3530.43,
      253.38,
      250.1,
      254.15,
      256.77,
      3257.6
    ],
    "mfu": [
      0,
      28.41,
      28.38,
      28.34,
      28.32,
      25.7,
      25.91,
      26.14,
      26.35,
      26.52,
      24.09,
      24.52,
      24.89,
      25.21,
      25.52,
      23.21,
      23.72,
      24.18,
      24.59,
      24.99,
      22.71,
      23.31,
      23.83,
      24.27,
      24.68,
      22.44,
      23.03,
      23.57,
      24.08,
      24.5,
      22.25,
      22.85,
      23.39,
      23.89,
      24.33,
      22.11,
      22.76,
      23.32,
      23.82,
      24.28,
      22.05,
      22.71,
      23.3,
      23.79,
      24.24,
      22.02,
      22.64,
      23.19,
      23.71,
      24.16,
      21.95,
      22.61,
      23.17,
      23.68,
      24.14,
      21.95,
      22.58,
      23.15,
      23.67,
      24.16,
      21.94,
      22.6,
      23.15,
      23.7,
      24.18,
      21.97,
      22.6,
      23.19,
      23.68,
      24.15,
      21.96,
      22.6,
      23.15,
      23.68,
      24.15,
      21.96,
      22.62,
      23.21,
      23.73,
      24.18,
      21.96,
      22.59,
      23.16,
      23.69,
      24.14,
      21.94,
      22.57,
      23.16,
      23.7,
      24.18,
      21.98,
      22.62,
      23.17,
      23.65,
      24.11,
      21.9,
      22.54,
      23.15,
      23.65,
      24.08,
      21.89
    ]
  },
  "model_info": {
    "parameters": "208.54M",
    "n_layer": "24",
    "n_head": "12",
    "n_embd": "768",
    "block_size": "512",
    "batch_size": "8",
    "learning_rate": 0.0003,
    "dropout": 0.1,
    "max_iters": "1000",
    "dataset": "shakespeare"
  }
}