{
  "validation": {
    "steps": [
      0,
      50,
      100,
      150,
      200,
      250,
      300,
      350,
      400,
      450,
      500,
      550,
      600,
      650,
      700,
      750,
      800,
      850,
      900,
      950,
      1000
    ],
    "train_loss": [
      10.8759,
      6.2134,
      5.0344,
      4.5797,
      4.1851,
      3.9941,
      3.8046,
      3.6167,
      3.4507,
      3.2675,
      3.1836,
      2.9348,
      2.7717,
      2.5757,
      2.3594,
      2.3212,
      2.1111,
      2.0316,
      1.913,
      1.9306,
      1.7602
    ],
    "val_loss": [
      10.8412,
      6.2526,
      5.3585,
      5.0656,
      4.9216,
      4.8444,
      4.6956,
      4.6772,
      4.5859,
      4.532,
      4.5865,
      4.6403,
      4.7261,
      4.875,
      4.895,
      4.8267,
      5.006,
      4.987,
      4.9088,
      5.0472,
      5.0743
    ]
  },
  "training": {
    "iters": [
      0,
      10,
      20,
      30,
      40,
      50,
      60,
      70,
      80,
      90,
      100,
      110,
      120,
      130,
      140,
      150,
      160,
      170,
      180,
      190,
      200,
      210,
      220,
      230,
      240,
      250,
      260,
      270,
      280,
      290,
      300,
      310,
      320,
      330,
      340,
      350,
      360,
      370,
      380,
      390,
      400,
      410,
      420,
      430,
      440,
      450,
      460,
      470,
      480,
      490,
      500,
      510,
      520,
      530,
      540,
      550,
      560,
      570,
      580,
      590,
      600,
      610,
      620,
      630,
      640,
      650,
      660,
      670,
      680,
      690,
      700,
      710,
      720,
      730,
      740,
      750,
      760,
      770,
      780,
      790,
      800,
      810,
      820,
      830,
      840,
      850,
      860,
      870,
      880,
      890,
      900,
      910,
      920,
      930,
      940,
      950,
      960,
      970,
      980,
      990,
      1000
    ],
    "loss": [
      10.8717,
      8.9779,
      8.4253,
      7.6397,
      6.8532,
      6.2461,
      5.8349,
      5.4443,
      5.2523,
      5.3401,
      5.0233,
      4.91,
      4.9217,
      4.6743,
      4.4009,
      4.331,
      4.6906,
      4.2135,
      4.4107,
      4.5274,
      4.347,
      4.3092,
      4.4049,
      4.1129,
      4.29,
      3.9,
      4.182,
      3.8365,
      3.891,
      4.218,
      3.9981,
      4.1827,
      3.701,
      3.4285,
      3.8845,
      3.4505,
      3.6345,
      3.4242,
      3.7007,
      3.505,
      3.3259,
      3.5976,
      3.2217,
      3.4763,
      3.3185,
      3.4921,
      3.4701,
      3.3562,
      3.3302,
      3.3956,
      3.5127,
      3.0098,
      3.0572,
      3.0777,
      3.1031,
      2.9884,
      3.1122,
      2.9807,
      2.977,
      2.9883,
      2.6274,
      3.1835,
      2.6848,
      2.8354,
      2.8321,
      2.9845,
      2.953,
      2.7901,
      2.8145,
      2.5795,
      2.5876,
      2.9247,
      2.6064,
      2.7498,
      2.5049,
      2.4035,
      2.3261,
      2.2877,
      2.4129,
      2.4803,
      2.3812,
      2.4651,
      2.3525,
      2.2468,
      2.2584,
      2.4466,
      2.3563,
      2.1583,
      2.3154,
      1.9652,
      1.9606,
      2.2272,
      1.9437,
      2.0401,
      2.5235,
      2.1521,
      2.0632,
      1.9736,
      2.1552,
      2.0651,
      2.1227
    ],
    "time": [
      16252.19,
      106.89,
      106.23,
      106.13,
      106.02,
      1328.67,
      106.25,
      107.21,
      106.02,
      107.89,
      1726.0,
      109.04,
      106.43,
      105.57,
      106.09,
      1720.58,
      106.3,
      107.02,
      109.33,
      107.23,
      1713.02,
      106.11,
      107.84,
      105.65,
      104.17,
      1698.53,
      108.37,
      104.5,
      111.37,
      108.57,
      1774.37,
      113.94,
      110.94,
      115.51,
      111.09,
      1741.64,
      109.08,
      110.48,
      113.67,
      113.57,
      1735.19,
      109.59,
      107.5,
      104.48,
      105.02,
      1706.59,
      102.0,
      103.56,
      110.2,
      103.37,
      1802.64,
      103.73,
      103.9,
      104.59,
      103.68,
      1755.64,
      103.48,
      102.46,
      103.56,
      105.08,
      1708.61,
      103.16,
      103.29,
      103.1,
      103.45,
      1708.51,
      106.04,
      106.96,
      102.24,
      101.33,
      1697.88,
      101.28,
      103.08,
      102.79,
      100.87,
      1711.96,
      100.23,
      113.16,
      103.85,
      116.62,
      1688.7,
      102.68,
      103.84,
      101.29,
      105.63,
      1719.28,
      109.61,
      102.28,
      110.29,
      110.41,
      1499.25,
      100.73,
      100.53,
      102.64,
      111.76,
      1743.46,
      103.35,
      113.32,
      114.16,
      113.4,
      1744.34
    ],
    "mfu": [
      0,
      18.91,
      18.92,
      18.93,
      18.95,
      17.2,
      17.39,
      17.53,
      17.69,
      17.79,
      16.13,
      16.37,
      16.63,
      16.88,
      17.1,
      15.51,
      15.86,
      16.16,
      16.39,
      16.64,
      15.09,
      15.49,
      15.81,
      16.15,
      16.47,
      14.94,
      15.31,
      15.72,
      15.96,
      16.23,
      14.72,
      15.02,
      15.34,
      15.56,
      15.82,
      14.35,
      14.77,
      15.12,
      15.39,
      15.63,
      14.18,
      14.61,
      15.03,
      15.46,
      15.84,
      14.37,
      14.92,
      15.38,
      15.67,
      16.06,
      14.57,
      15.06,
      15.5,
      15.88,
      16.24,
      14.73,
      15.21,
      15.67,
      16.05,
      16.37,
      14.85,
      15.32,
      15.75,
      16.13,
      16.48,
      14.95,
      15.36,
      15.71,
      16.12,
      16.5,
      14.97,
      15.47,
      15.88,
      16.26,
      16.64,
      15.09,
      15.6,
      15.83,
      16.19,
      16.3,
      14.79,
      15.28,
      15.7,
      16.13,
      16.43,
      14.9,
      15.26,
      15.71,
      15.97,
      16.2,
      14.72,
      15.25,
      15.74,
      16.13,
      16.33,
      14.81,
      15.29,
      15.54,
      15.76,
      15.96,
      14.48
    ]
  },
  "model_info": {
    "parameters": "123.59M",
    "n_layer": "12",
    "n_head": "12",
    "n_embd": "768",
    "block_size": "256",
    "batch_size": "8",
    "learning_rate": 0.0003,
    "dropout": 0.1,
    "max_iters": "1000",
    "dataset": "shakespeare"
  }
}