{
  "validation": {
    "steps": [
      0,
      50,
      100,
      150,
      200,
      250,
      300,
      350,
      400,
      450,
      500,
      550,
      600,
      650,
      700,
      750,
      800,
      850,
      900,
      950,
      1000
    ],
    "train_loss": [
      10.875,
      6.1965,
      5.023,
      4.6004,
      4.2273,
      4.0672,
      3.9253,
      3.773,
      3.6454,
      3.5159,
      3.4843,
      3.2913,
      3.21,
      3.0722,
      2.9433,
      2.9535,
      2.7986,
      2.7576,
      2.6869,
      2.7193,
      2.5741
    ],
    "val_loss": [
      10.8399,
      6.2566,
      5.3592,
      5.0766,
      4.9731,
      4.9118,
      4.7943,
      4.7861,
      4.7418,
      4.6664,
      4.6949,
      4.7287,
      4.7926,
      4.926,
      4.8872,
      4.7949,
      4.9576,
      4.9171,
      4.8264,
      4.9467,
      4.9424
    ]
  },
  "training": {
    "iters": [
      0,
      10,
      20,
      30,
      40,
      50,
      60,
      70,
      80,
      90,
      100,
      110,
      120,
      130,
      140,
      150,
      160,
      170,
      180,
      190,
      200,
      210,
      220,
      230,
      240,
      250,
      260,
      270,
      280,
      290,
      300,
      310,
      320,
      330,
      340,
      350,
      360,
      370,
      380,
      390,
      400,
      410,
      420,
      430,
      440,
      450,
      460,
      470,
      480,
      490,
      500,
      510,
      520,
      530,
      540,
      550,
      560,
      570,
      580,
      590,
      600,
      610,
      620,
      630,
      640,
      650,
      660,
      670,
      680,
      690,
      700,
      710,
      720,
      730,
      740,
      750,
      760,
      770,
      780,
      790,
      800,
      810,
      820,
      830,
      840,
      850,
      860,
      870,
      880,
      890,
      900,
      910,
      920,
      930,
      940,
      950,
      960,
      970,
      980,
      990,
      1000
    ],
    "loss": [
      10.8878,
      8.9774,
      8.4226,
      7.6484,
      6.8547,
      6.2433,
      5.8404,
      5.4244,
      5.2187,
      5.3286,
      5.0384,
      4.9024,
      4.945,
      4.6968,
      4.4223,
      4.3635,
      4.6863,
      4.2392,
      4.4462,
      4.565,
      4.3777,
      4.3465,
      4.434,
      4.1783,
      4.3304,
      3.9865,
      4.2604,
      3.9241,
      3.9955,
      4.266,
      4.0818,
      4.3059,
      3.8768,
      3.5841,
      3.9972,
      3.6097,
      3.7986,
      3.6037,
      3.8969,
      3.6672,
      3.5226,
      3.779,
      3.4233,
      3.7027,
      3.5214,
      3.7153,
      3.7105,
      3.59,
      3.6182,
      3.6575,
      3.7645,
      3.2984,
      3.4136,
      3.3494,
      3.4466,
      3.2964,
      3.4506,
      3.3646,
      3.3545,
      3.3952,
      3.018,
      3.6824,
      3.1257,
      3.2629,
      3.2672,
      3.4752,
      3.4024,
      3.2614,
      3.29,
      3.0559,
      3.0615,
      3.3727,
      3.092,
      3.278,
      3.0398,
      2.9768,
      2.8703,
      2.8456,
      2.998,
      3.1208,
      3.069,
      3.0289,
      2.9373,
      2.9071,
      2.8221,
      3.1061,
      2.9344,
      2.8432,
      2.9502,
      2.6747,
      2.5972,
      2.8869,
      2.5869,
      2.6858,
      3.15,
      2.8268,
      2.7028,
      2.6685,
      2.8098,
      2.7706,
      2.8587
    ],
    "time": [
      958.95,
      153.99,
      146.43,
      151.06,
      155.92,
      1528.73,
      149.43,
      146.41,
      152.98,
      150.58,
      1981.77,
      137.79,
      133.66,
      131.75,
      131.42,
      1864.05,
      138.18,
      136.43,
      137.18,
      135.46,
      1919.26,
      131.92,
      129.64,
      130.17,
      130.3,
      1867.82,
      140.33,
      130.67,
      129.46,
      131.33,
      1838.0,
      132.61,
      142.41,
      140.53,
      130.36,
      1831.42,
      127.8,
      129.08,
      128.24,
      129.58,
      1849.82,
      130.97,
      129.55,
      130.96,
      128.16,
      1848.27,
      141.35,
      132.57,
      131.47,
      129.2,
      1855.56,
      130.43,
      140.08,
      140.83,
      130.51,
      1872.69,
      130.35,
      130.52,
      130.13,
      130.43,
      1820.57,
      130.75,
      130.25,
      132.05,
      131.22,
      1910.28,
      136.23,
      138.87,
      134.42,
      137.77,
      1694.53,
      134.44,
      130.73,
      129.65,
      137.79,
      1842.42,
      129.89,
      135.16,
      153.05,
      148.16,
      1873.0,
      130.45,
      139.86,
      135.53,
      131.73,
      1836.98,
      131.0,
      136.31,
      150.67,
      150.85,
      1911.36,
      149.29,
      149.77,
      151.05,
      149.76,
      1908.31,
      147.16,
      149.93,
      145.89,
      145.83,
      1937.38
    ],
    "mfu": [
      0,
      13.13,
      13.19,
      13.21,
      13.19,
      12.0,
      12.15,
      12.32,
      12.41,
      12.51,
      11.36,
      11.69,
      12.03,
      12.37,
      12.67,
      11.51,
      11.82,
      12.12,
      12.38,
      12.64,
      11.48,
      11.86,
      12.23,
      12.56,
      12.86,
      11.68,
      11.95,
      12.31,
      12.64,
      12.91,
      11.73,
      12.08,
      12.29,
      12.5,
      12.8,
      11.63,
      12.05,
      12.41,
      12.75,
      13.03,
      11.84,
      12.2,
      12.54,
      12.83,
      13.12,
      11.92,
      12.16,
      12.47,
      12.76,
      13.05,
      11.85,
      12.21,
      12.44,
      12.63,
      12.91,
      11.73,
      12.11,
      12.45,
      12.75,
      13.03,
      11.84,
      12.2,
      12.53,
      12.81,
      13.07,
      11.87,
      12.16,
      12.4,
      12.67,
      12.87,
      11.7,
      12.03,
      12.38,
      12.7,
      12.89,
      11.72,
      12.1,
      12.39,
      12.47,
      12.58,
      11.43,
      11.84,
      12.1,
      12.38,
      12.68,
      11.52,
      11.91,
      12.2,
      12.32,
      12.43,
      11.29,
      11.52,
      11.72,
      11.88,
      12.04,
      10.95,
      11.23,
      11.45,
      11.69,
      11.91,
      10.82
    ]
  },
  "model_info": {
    "parameters": "123.59M",
    "n_layer": "12",
    "n_head": "12",
    "n_embd": "768",
    "block_size": "256",
    "batch_size": "8",
    "learning_rate": 0.0003,
    "dropout": 0.1,
    "max_iters": "1000",
    "dataset": "shakespeare"
  }
}