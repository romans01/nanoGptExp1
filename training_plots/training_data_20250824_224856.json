{
  "validation": {
    "steps": [
      0,
      50,
      100,
      150,
      200,
      250,
      300,
      350,
      400,
      450,
      500,
      550,
      600,
      650,
      700,
      750,
      800,
      850,
      900,
      950,
      1000
    ],
    "train_loss": [
      10.9108,
      6.0385,
      4.9178,
      4.5222,
      4.1761,
      3.9914,
      3.8222,
      3.6451,
      3.4921,
      3.3286,
      3.2565,
      3.0478,
      2.9033,
      2.7393,
      2.5398,
      2.5092,
      2.3261,
      2.2529,
      2.1448,
      2.1652,
      2.0011
    ],
    "val_loss": [
      10.8764,
      6.1414,
      5.3031,
      5.0339,
      4.9708,
      4.8852,
      4.7309,
      4.7156,
      4.6456,
      4.5727,
      4.6014,
      4.6601,
      4.722,
      4.8755,
      4.8945,
      4.8155,
      4.9699,
      4.9599,
      4.8828,
      4.9942,
      5.0358
    ]
  },
  "training": {
    "iters": [
      0,
      10,
      20,
      30,
      40,
      50,
      60,
      70,
      80,
      90,
      100,
      110,
      120,
      130,
      140,
      150,
      160,
      170,
      180,
      190,
      200,
      210,
      220,
      230,
      240,
      250,
      260,
      270,
      280,
      290,
      300,
      310,
      320,
      330,
      340,
      350,
      360,
      370,
      380,
      390,
      400,
      410,
      420,
      430,
      440,
      450,
      460,
      470,
      480,
      490,
      500,
      510,
      520,
      530,
      540,
      550,
      560,
      570,
      580,
      590,
      600,
      610,
      620,
      630,
      640,
      650,
      660,
      670,
      680,
      690,
      700,
      710,
      720,
      730,
      740,
      750,
      760,
      770,
      780,
      790,
      800,
      810,
      820,
      830,
      840,
      850,
      860,
      870,
      880,
      890,
      900,
      910,
      920,
      930,
      940,
      950,
      960,
      970,
      980,
      990,
      1000
    ],
    "loss": [
      10.9106,
      8.9796,
      8.377,
      7.5981,
      6.8072,
      6.109,
      5.6063,
      5.1876,
      5.0738,
      5.1781,
      4.8996,
      4.81,
      4.8675,
      4.6149,
      4.3441,
      4.2774,
      4.6209,
      4.1966,
      4.3742,
      4.5052,
      4.3344,
      4.3086,
      4.3861,
      4.1,
      4.2298,
      3.9135,
      4.1707,
      3.8241,
      3.9016,
      4.1743,
      3.9995,
      4.1775,
      3.7568,
      3.4424,
      3.8667,
      3.4607,
      3.63,
      3.4501,
      3.7201,
      3.54,
      3.3485,
      3.5981,
      3.2533,
      3.5166,
      3.3528,
      3.5455,
      3.5175,
      3.4013,
      3.415,
      3.4565,
      3.55,
      3.0974,
      3.1371,
      3.1462,
      3.1791,
      3.059,
      3.2111,
      3.0714,
      3.1206,
      3.1068,
      2.7393,
      3.3367,
      2.8081,
      2.9552,
      2.9374,
      3.0959,
      3.0922,
      2.9348,
      2.9074,
      2.6964,
      2.7282,
      3.0256,
      2.7706,
      2.9088,
      2.628,
      2.59,
      2.4617,
      2.4296,
      2.5857,
      2.6383,
      2.5723,
      2.6381,
      2.5278,
      2.4761,
      2.4387,
      2.6072,
      2.4955,
      2.3577,
      2.517,
      2.2154,
      2.1606,
      2.406,
      2.0861,
      2.1929,
      2.6942,
      2.2906,
      2.2484,
      2.1691,
      2.3517,
      2.2907,
      2.3315
    ],
    "time": [
      853.32,
      127.86,
      129.55,
      126.4,
      131.1,
      1473.37,
      127.91,
      126.32,
      129.79,
      128.34,
      1869.61,
      126.21,
      128.01,
      127.5,
      128.88,
      1853.37,
      133.01,
      133.97,
      133.44,
      133.25,
      1908.29,
      130.42,
      133.37,
      134.26,
      131.93,
      1850.14,
      127.16,
      127.03,
      127.7,
      125.28,
      1843.43,
      124.23,
      118.93,
      120.79,
      118.08,
      1822.09,
      118.54,
      118.66,
      117.65,
      117.47,
      1787.5,
      117.16,
      116.76,
      117.33,
      121.28,
      1822.8,
      122.39,
      132.26,
      124.99,
      122.17,
      1819.98,
      121.27,
      120.0,
      132.62,
      141.74,
      1856.35,
      127.36,
      128.89,
      140.95,
      128.92,
      1876.61,
      127.76,
      135.32,
      120.75,
      120.66,
      1847.87,
      137.7,
      136.38,
      132.86,
      121.54,
      1831.34,
      121.7,
      131.76,
      126.91,
      126.03,
      1877.82,
      130.19,
      123.25,
      126.56,
      133.03,
      1856.62,
      127.77,
      126.28,
      135.88,
      126.89,
      1835.63,
      129.58,
      128.21,
      125.78,
      134.16,
      1864.85,
      127.11,
      132.06,
      136.94,
      126.04,
      1806.92,
      126.98,
      122.73,
      118.68,
      119.1,
      1856.02
    ],
    "mfu": [
      0,
      15.81,
      15.79,
      15.81,
      15.77,
      14.33,
      14.48,
      14.63,
      14.72,
      14.83,
      13.45,
      13.71,
      13.92,
      14.11,
      14.27,
      12.95,
      13.17,
      13.37,
      13.54,
      13.71,
      12.44,
      12.75,
      12.99,
      13.19,
      13.41,
      12.18,
      12.55,
      12.88,
      13.18,
      13.47,
      12.24,
      12.64,
      13.08,
      13.44,
      13.81,
      12.54,
      12.99,
      13.39,
      13.77,
      14.12,
      12.82,
      13.26,
      13.67,
      14.02,
      14.29,
      12.97,
      13.32,
      13.52,
      13.78,
      14.06,
      12.77,
      13.16,
      13.52,
      13.7,
      13.75,
      12.49,
      12.82,
      13.11,
      13.23,
      13.48,
      12.24,
      12.6,
      12.83,
      13.22,
      13.57,
      12.33,
      12.56,
      12.79,
      13.03,
      13.39,
      12.16,
      12.61,
      12.88,
      13.18,
      13.47,
      12.23,
      12.56,
      12.94,
      13.25,
      13.44,
      12.21,
      12.57,
      12.91,
      13.11,
      13.39,
      12.16,
      12.51,
      12.83,
      13.16,
      13.35,
      12.12,
      12.5,
      12.78,
      12.98,
      13.28,
      12.07,
      12.45,
      12.85,
      13.27,
      13.64,
      12.39
    ]
  },
  "model_info": {
    "parameters": "123.59M",
    "n_layer": "12",
    "n_head": "12",
    "n_embd": "768",
    "block_size": "256",
    "batch_size": "8",
    "learning_rate": 0.0003,
    "dropout": 0.1,
    "max_iters": "1000",
    "dataset": "shakespeare"
  }
}