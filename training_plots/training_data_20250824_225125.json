{
  "validation": {
    "steps": [
      0,
      50,
      100,
      150,
      200,
      250,
      300,
      350,
      400,
      450,
      500,
      550,
      600,
      650,
      700,
      750,
      800,
      850,
      900,
      950,
      1000
    ],
    "train_loss": [
      10.8787,
      6.1142,
      4.9759,
      4.5548,
      4.1656,
      4.0009,
      3.8276,
      3.645,
      3.4979,
      3.3192,
      3.2558,
      3.0301,
      2.8965,
      2.719,
      2.5174,
      2.4938,
      2.3092,
      2.2366,
      2.12,
      2.1501,
      1.9894
    ],
    "val_loss": [
      10.8414,
      6.1928,
      5.3364,
      5.0593,
      4.922,
      4.8687,
      4.7129,
      4.6833,
      4.6113,
      4.5323,
      4.5733,
      4.6185,
      4.6939,
      4.827,
      4.8142,
      4.7416,
      4.9013,
      4.8721,
      4.7771,
      4.9121,
      4.93
    ]
  },
  "training": {
    "iters": [
      0,
      10,
      20,
      30,
      40,
      50,
      60,
      70,
      80,
      90,
      100,
      110,
      120,
      130,
      140,
      150,
      160,
      170,
      180,
      190,
      200,
      210,
      220,
      230,
      240,
      250,
      260,
      270,
      280,
      290,
      300,
      310,
      320,
      330,
      340,
      350,
      360,
      370,
      380,
      390,
      400,
      410,
      420,
      430,
      440,
      450,
      460,
      470,
      480,
      490,
      500,
      510,
      520,
      530,
      540,
      550,
      560,
      570,
      580,
      590,
      600,
      610,
      620,
      630,
      640,
      650,
      660,
      670,
      680,
      690,
      700,
      710,
      720,
      730,
      740,
      750,
      760,
      770,
      780,
      790,
      800,
      810,
      820,
      830,
      840,
      850,
      860,
      870,
      880,
      890,
      900,
      910,
      920,
      930,
      940,
      950,
      960,
      970,
      980,
      990,
      1000
    ],
    "loss": [
      10.9426,
      8.974,
      8.3863,
      7.6135,
      6.812,
      6.1694,
      5.7249,
      5.306,
      5.1299,
      5.2387,
      4.9612,
      4.8526,
      4.8963,
      4.6292,
      4.3826,
      4.307,
      4.6724,
      4.1961,
      4.3834,
      4.5291,
      4.3384,
      4.299,
      4.3934,
      4.0962,
      4.2678,
      3.926,
      4.2068,
      3.8394,
      3.9232,
      4.2207,
      4.022,
      4.2198,
      3.7789,
      3.4486,
      3.9133,
      3.5066,
      3.6714,
      3.4479,
      3.7654,
      3.559,
      3.3666,
      3.6499,
      3.263,
      3.534,
      3.367,
      3.5483,
      3.5314,
      3.3976,
      3.4448,
      3.4354,
      3.5682,
      3.092,
      3.1672,
      3.1339,
      3.1933,
      3.0556,
      3.2084,
      3.0841,
      3.0897,
      3.0817,
      2.7124,
      3.3204,
      2.809,
      2.9668,
      2.9191,
      3.099,
      3.0728,
      2.9075,
      2.9205,
      2.709,
      2.7089,
      3.0495,
      2.7578,
      2.8877,
      2.6535,
      2.576,
      2.4534,
      2.4273,
      2.5703,
      2.6422,
      2.5794,
      2.629,
      2.5052,
      2.4317,
      2.4374,
      2.6095,
      2.4971,
      2.3532,
      2.4962,
      2.1852,
      2.1573,
      2.4102,
      2.0797,
      2.1885,
      2.7059,
      2.3094,
      2.2455,
      2.1601,
      2.348,
      2.2686,
      2.3253
    ],
    "time": [
      873.55,
      108.18,
      109.46,
      109.6,
      111.1,
      1406.84,
      110.61,
      110.27,
      110.93,
      109.31,
      1819.77,
      109.84,
      110.33,
      111.79,
      109.87,
      1787.67,
      115.13,
      110.66,
      110.16,
      111.19,
      1808.15,
      109.89,
      109.89,
      110.2,
      110.1,
      1805.23,
      110.79,
      109.94,
      111.99,
      108.16,
      1796.73,
      111.12,
      111.07,
      109.88,
      108.52,
      1824.76,
      111.06,
      110.54,
      111.33,
      110.23,
      1815.33,
      112.22,
      107.85,
      110.02,
      109.51,
      1807.7,
      109.99,
      108.75,
      108.97,
      110.13,
      1794.62,
      110.9,
      109.86,
      110.43,
      110.54,
      1868.82,
      110.5,
      110.03,
      108.69,
      109.95,
      1799.89,
      111.28,
      110.47,
      111.39,
      109.11,
      1858.76,
      108.86,
      108.1,
      109.87,
      108.37,
      1873.21,
      109.82,
      110.76,
      109.65,
      112.57,
      1800.52,
      107.96,
      109.67,
      109.35,
      107.52,
      1805.96,
      110.08,
      109.39,
      110.95,
      111.25,
      1827.09,
      109.01,
      110.51,
      111.44,
      111.75,
      1809.82,
      110.04,
      111.66,
      109.2,
      110.3,
      1807.27,
      108.6,
      110.17,
      109.91,
      109.56,
      1804.91
    ],
    "mfu": [
      0,
      18.68,
      18.66,
      18.64,
      18.6,
      16.88,
      17.02,
      17.15,
      17.26,
      17.38,
      15.75,
      16.02,
      16.25,
      16.43,
      16.63,
      15.08,
      15.33,
      15.62,
      15.89,
      16.12,
      14.62,
      15.0,
      15.34,
      15.64,
      15.91,
      14.43,
      14.81,
      15.17,
      15.46,
      15.78,
      14.32,
      14.7,
      15.05,
      15.39,
      15.71,
      14.25,
      14.65,
      15.01,
      15.32,
      15.63,
      14.17,
      14.56,
      14.98,
      15.32,
      15.63,
      14.18,
      14.6,
      15.0,
      15.35,
      15.65,
      14.2,
      14.6,
      14.98,
      15.31,
      15.61,
      14.16,
      14.57,
      14.95,
      15.32,
      15.62,
      14.17,
      14.57,
      14.94,
      15.26,
      15.59,
      14.14,
      14.58,
      14.99,
      15.34,
      15.67,
      14.21,
      14.63,
      14.99,
      15.33,
      15.6,
      14.15,
      14.61,
      14.99,
      15.34,
      15.68,
      14.23,
      14.64,
      15.03,
      15.34,
      15.63,
      14.17,
      14.61,
      14.98,
      15.3,
      15.57,
      14.13,
      14.55,
      14.91,
      15.27,
      15.57,
      14.13,
      14.58,
      14.95,
      15.3,
      15.61,
      14.16
    ]
  },
  "model_info": {
    "parameters": "123.59M",
    "n_layer": "12",
    "n_head": "12",
    "n_embd": "768",
    "block_size": "256",
    "batch_size": "8",
    "learning_rate": 0.0003,
    "dropout": 0.1,
    "max_iters": "1000",
    "dataset": "shakespeare"
  }
}