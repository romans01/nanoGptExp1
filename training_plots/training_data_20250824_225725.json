{
  "validation": {
    "steps": [
      0,
      50,
      100,
      150,
      200,
      250,
      300,
      350,
      400,
      450,
      500,
      550,
      600,
      650,
      700,
      750,
      800,
      850,
      900,
      950,
      1000
    ],
    "train_loss": [
      10.8779,
      6.1057,
      4.9763,
      4.5676,
      4.2138,
      4.0713,
      3.9399,
      3.7907,
      3.6659,
      3.5305,
      3.5068,
      3.3287,
      3.265,
      3.1472,
      3.0302,
      3.0542,
      2.915,
      2.8891,
      2.8371,
      2.8572,
      2.7343
    ],
    "val_loss": [
      10.8405,
      6.1826,
      5.3358,
      5.0687,
      4.9577,
      4.9291,
      4.8086,
      4.8152,
      4.7186,
      4.659,
      4.7034,
      4.7379,
      4.7896,
      4.9019,
      4.8737,
      4.7959,
      4.9382,
      4.8802,
      4.7933,
      4.8917,
      4.8833
    ]
  },
  "training": {
    "iters": [
      0,
      10,
      20,
      30,
      40,
      50,
      60,
      70,
      80,
      90,
      100,
      110,
      120,
      130,
      140,
      150,
      160,
      170,
      180,
      190,
      200,
      210,
      220,
      230,
      240,
      250,
      260,
      270,
      280,
      290,
      300,
      310,
      320,
      330,
      340,
      350,
      360,
      370,
      380,
      390,
      400,
      410,
      420,
      430,
      440,
      450,
      460,
      470,
      480,
      490,
      500,
      510,
      520,
      530,
      540,
      550,
      560,
      570,
      580,
      590,
      600,
      610,
      620,
      630,
      640,
      650,
      660,
      670,
      680,
      690,
      700,
      710,
      720,
      730,
      740,
      750,
      760,
      770,
      780,
      790,
      800,
      810,
      820,
      830,
      840,
      850,
      860,
      870,
      880,
      890,
      900,
      910,
      920,
      930,
      940,
      950,
      960,
      970,
      980,
      990,
      1000
    ],
    "loss": [
      10.9411,
      8.9734,
      8.387,
      7.6149,
      6.8216,
      6.158,
      5.6926,
      5.3105,
      5.1409,
      5.2649,
      4.975,
      4.8649,
      4.9075,
      4.6508,
      4.4157,
      4.3392,
      4.7062,
      4.2282,
      4.4274,
      4.5614,
      4.389,
      4.3432,
      4.4536,
      4.1803,
      4.3427,
      4.0023,
      4.3436,
      3.9448,
      4.0098,
      4.3039,
      4.1169,
      4.3645,
      3.9222,
      3.5943,
      4.0432,
      3.6933,
      3.9059,
      3.6029,
      3.9654,
      3.724,
      3.5654,
      3.8134,
      3.4512,
      3.7569,
      3.5983,
      3.8006,
      3.7606,
      3.6444,
      3.7366,
      3.694,
      3.809,
      3.3462,
      3.4587,
      3.4124,
      3.497,
      3.357,
      3.5081,
      3.4238,
      3.4341,
      3.4636,
      3.0618,
      3.7275,
      3.1811,
      3.3201,
      3.3635,
      3.5369,
      3.4691,
      3.3696,
      3.4043,
      3.1659,
      3.191,
      3.4567,
      3.218,
      3.3794,
      3.189,
      3.086,
      3.0052,
      3.0074,
      3.1333,
      3.2492,
      3.2109,
      3.1916,
      3.062,
      3.0562,
      2.948,
      3.2414,
      3.0428,
      2.9593,
      3.1135,
      2.8371,
      2.7883,
      3.0028,
      2.7537,
      2.8624,
      3.2717,
      3.001,
      2.8271,
      2.8197,
      2.9551,
      2.9591,
      3.0144
    ],
    "time": [
      932.26,
      135.29,
      134.38,
      132.36,
      133.65,
      1443.16,
      132.02,
      134.42,
      132.33,
      133.68,
      1927.63,
      133.06,
      136.4,
      132.37,
      134.24,
      1916.08,
      133.88,
      133.05,
      139.33,
      132.87,
      1836.97,
      136.0,
      134.47,
      133.57,
      134.16,
      1817.13,
      134.75,
      133.43,
      134.94,
      134.24,
      1834.3,
      134.47,
      133.68,
      131.06,
      134.96,
      1899.82,
      132.67,
      134.83,
      133.91,
      134.07,
      1913.8,
      132.8,
      132.71,
      134.66,
      136.58,
      1861.14,
      134.32,
      133.09,
      135.25,
      135.22,
      1896.66,
      135.02,
      132.37,
      132.7,
      141.85,
      1887.12,
      140.01,
      137.11,
      136.68,
      134.04,
      1823.81,
      136.66,
      137.4,
      133.67,
      131.93,
      1835.18,
      131.01,
      128.67,
      129.68,
      132.04,
      1870.58,
      131.25,
      133.46,
      132.3,
      130.67,
      1830.73,
      132.75,
      130.49,
      131.18,
      126.72,
      1825.74,
      132.49,
      132.06,
      132.12,
      130.92,
      1821.46,
      130.61,
      130.15,
      129.16,
      131.45,
      1898.18,
      129.96,
      130.03,
      133.14,
      131.31,
      1908.27,
      132.14,
      133.94,
      137.28,
      132.93,
      1827.24
    ],
    "mfu": [
      0,
      14.94,
      14.95,
      14.98,
      15.0,
      13.64,
      13.8,
      13.93,
      14.06,
      14.17,
      12.86,
      13.09,
      13.26,
      13.46,
      13.62,
      12.37,
      12.64,
      12.89,
      13.06,
      13.27,
      12.05,
      12.34,
      12.6,
      12.86,
      13.08,
      11.88,
      12.19,
      12.49,
      12.74,
      12.97,
      11.78,
      12.11,
      12.41,
      12.71,
      12.94,
      11.75,
      12.1,
      12.39,
      12.66,
      12.9,
      11.72,
      12.07,
      12.38,
      12.65,
      12.86,
      11.68,
      12.02,
      12.34,
      12.6,
      12.83,
      11.66,
      11.99,
      12.32,
      12.61,
      12.77,
      11.6,
      11.89,
      12.17,
      12.43,
      12.7,
      11.54,
      11.86,
      12.15,
      12.45,
      12.73,
      11.57,
      11.96,
      12.33,
      12.66,
      12.92,
      11.74,
      12.1,
      12.41,
      12.7,
      12.97,
      11.79,
      12.13,
      12.47,
      12.76,
      13.08,
      11.88,
      12.22,
      12.53,
      12.81,
      13.07,
      11.87,
      12.23,
      12.56,
      12.87,
      13.12,
      11.92,
      12.28,
      12.61,
      12.86,
      13.12,
      11.91,
      12.25,
      12.53,
      12.75,
      13.0,
      11.81
    ]
  },
  "model_info": {
    "parameters": "123.59M",
    "n_layer": "12",
    "n_head": "12",
    "n_embd": "768",
    "block_size": "256",
    "batch_size": "8",
    "learning_rate": 0.0003,
    "dropout": 0.1,
    "max_iters": "1000",
    "dataset": "shakespeare"
  }
}