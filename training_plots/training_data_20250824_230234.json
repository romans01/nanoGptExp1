{
  "validation": {
    "steps": [
      0,
      50,
      100,
      150,
      200,
      250,
      300,
      350,
      400,
      450,
      500,
      550,
      600,
      650,
      700,
      750,
      800,
      850,
      900,
      950,
      1000
    ],
    "train_loss": [
      10.9111,
      6.0399,
      4.9127,
      4.5402,
      4.2202,
      4.0791,
      3.9497,
      3.8147,
      3.6973,
      3.567,
      3.5475,
      3.3736,
      3.2947,
      3.1761,
      3.0582,
      3.0654,
      2.9234,
      2.89,
      2.8278,
      2.8498,
      2.7234
    ],
    "val_loss": [
      10.8763,
      6.1441,
      5.2996,
      5.0468,
      4.9939,
      4.9595,
      4.8477,
      4.8643,
      4.8072,
      4.7434,
      4.7847,
      4.8347,
      4.8757,
      5.0048,
      4.9762,
      4.87,
      5.018,
      4.9816,
      4.8767,
      4.9878,
      4.9845
    ]
  },
  "training": {
    "iters": [
      0,
      10,
      20,
      30,
      40,
      50,
      60,
      70,
      80,
      90,
      100,
      110,
      120,
      130,
      140,
      150,
      160,
      170,
      180,
      190,
      200,
      210,
      220,
      230,
      240,
      250,
      260,
      270,
      280,
      290,
      300,
      310,
      320,
      330,
      340,
      350,
      360,
      370,
      380,
      390,
      400,
      410,
      420,
      430,
      440,
      450,
      460,
      470,
      480,
      490,
      500,
      510,
      520,
      530,
      540,
      550,
      560,
      570,
      580,
      590,
      600,
      610,
      620,
      630,
      640,
      650,
      660,
      670,
      680,
      690,
      700,
      710,
      720,
      730,
      740,
      750,
      760,
      770,
      780,
      790,
      800,
      810,
      820,
      830,
      840,
      850,
      860,
      870,
      880,
      890,
      900,
      910,
      920,
      930,
      940,
      950,
      960,
      970,
      980,
      990,
      1000
    ],
    "loss": [
      10.9107,
      8.9792,
      8.3772,
      7.5992,
      6.8067,
      6.114,
      5.6044,
      5.1818,
      5.0737,
      5.1798,
      4.8934,
      4.8136,
      4.8646,
      4.632,
      4.3631,
      4.3017,
      4.6513,
      4.224,
      4.4035,
      4.5312,
      4.3757,
      4.3415,
      4.426,
      4.1584,
      4.2919,
      4.008,
      4.2495,
      3.9093,
      4.0037,
      4.2417,
      4.1215,
      4.2843,
      3.9099,
      3.6154,
      4.0448,
      3.6699,
      3.8075,
      3.6349,
      3.9178,
      3.7321,
      3.5536,
      3.8,
      3.4929,
      3.7542,
      3.5906,
      3.7722,
      3.753,
      3.6696,
      3.6493,
      3.6981,
      3.815,
      3.3554,
      3.4625,
      3.4195,
      3.5016,
      3.3554,
      3.5024,
      3.4529,
      3.4479,
      3.4616,
      3.098,
      3.7053,
      3.197,
      3.3235,
      3.3351,
      3.5246,
      3.4766,
      3.3277,
      3.3548,
      3.127,
      3.1375,
      3.445,
      3.1955,
      3.354,
      3.1424,
      3.1207,
      2.9724,
      2.9621,
      3.0943,
      3.1745,
      3.1618,
      3.1251,
      3.0224,
      3.0385,
      2.9206,
      3.193,
      3.0244,
      2.9316,
      3.0714,
      2.8252,
      2.7297,
      2.9859,
      2.7112,
      2.8199,
      3.2121,
      2.9181,
      2.7917,
      2.7748,
      2.8914,
      2.8983,
      2.9599
    ],
    "time": [
      802.89,
      119.82,
      121.03,
      117.5,
      118.18,
      1374.21,
      121.09,
      117.61,
      117.22,
      117.31,
      1803.48,
      117.44,
      117.19,
      117.29,
      117.43,
      1756.49,
      118.35,
      112.29,
      117.05,
      117.31,
      1750.97,
      117.21,
      117.66,
      120.92,
      117.19,
      1776.68,
      118.1,
      117.31,
      117.34,
      117.61,
      1766.27,
      117.07,
      117.5,
      119.92,
      117.27,
      1780.43,
      117.43,
      117.61,
      117.31,
      120.43,
      1820.82,
      117.78,
      121.15,
      118.07,
      117.45,
      1763.99,
      117.51,
      117.21,
      117.18,
      120.6,
      1754.8,
      117.87,
      119.06,
      117.79,
      117.34,
      1810.87,
      117.24,
      117.23,
      117.12,
      118.72,
      1751.27,
      119.32,
      117.34,
      117.54,
      117.4,
      1819.07,
      117.39,
      117.52,
      121.06,
      117.69,
      1753.24,
      117.39,
      117.54,
      117.9,
      117.77,
      1498.58,
      117.59,
      118.37,
      117.42,
      120.74,
      1817.86,
      117.77,
      117.57,
      118.09,
      117.41,
      1813.57,
      117.4,
      117.32,
      119.98,
      117.85,
      1741.76,
      117.17,
      117.58,
      117.34,
      117.5,
      1744.32,
      117.37,
      117.88,
      117.27,
      117.85,
      1763.21
    ],
    "mfu": [
      0,
      16.87,
      16.85,
      16.89,
      16.91,
      15.37,
      15.5,
      15.67,
      15.82,
      15.97,
      14.48,
      14.75,
      15.0,
      15.23,
      15.43,
      14.0,
      14.31,
      14.68,
      14.93,
      15.16,
      13.76,
      14.11,
      14.42,
      14.65,
      14.91,
      13.53,
      13.89,
      14.22,
      14.52,
      14.79,
      13.43,
      13.81,
      14.15,
      14.42,
      14.7,
      13.34,
      13.73,
      14.08,
      14.39,
      14.63,
      13.28,
      13.67,
      13.97,
      14.28,
      14.58,
      13.23,
      13.63,
      13.99,
      14.32,
      14.56,
      13.22,
      13.61,
      13.95,
      14.27,
      14.57,
      13.22,
      13.62,
      13.99,
      14.31,
      14.58,
      13.24,
      13.61,
      13.97,
      14.29,
      14.59,
      13.24,
      13.64,
      13.99,
      14.26,
      14.56,
      13.21,
      13.62,
      13.97,
      14.29,
      14.58,
      13.25,
      13.65,
      13.99,
      14.31,
      14.56,
      13.21,
      13.61,
      13.97,
      14.28,
      14.57,
      13.23,
      13.63,
      13.99,
      14.27,
      14.56,
      13.22,
      13.62,
      13.98,
      14.31,
      14.6,
      13.25,
      13.65,
      14.0,
      14.32,
      14.61,
      13.26
    ]
  },
  "model_info": {
    "parameters": "123.59M",
    "n_layer": "12",
    "n_head": "12",
    "n_embd": "768",
    "block_size": "256",
    "batch_size": "8",
    "learning_rate": 0.0003,
    "dropout": 0.1,
    "max_iters": "1000",
    "dataset": "shakespeare"
  }
}