{
  "validation": {
    "steps": [
      0,
      50,
      100,
      150,
      200,
      250,
      300,
      350,
      400,
      450,
      500,
      550,
      600,
      650,
      700,
      750,
      800,
      850,
      900,
      950,
      1000
    ],
    "train_loss": [
      10.872,
      6.2938,
      5.4527,
      4.8922,
      4.4381,
      4.2053,
      4.0471,
      3.8471,
      3.7027,
      3.5433,
      3.4795,
      3.2712,
      3.1664,
      3.0138,
      2.8418,
      2.842,
      2.6589,
      2.6097,
      2.5124,
      2.5509,
      2.3908
    ],
    "val_loss": [
      10.8582,
      6.33,
      5.6403,
      5.2395,
      5.0509,
      4.9125,
      4.7592,
      4.6747,
      4.5677,
      4.4967,
      4.5093,
      4.5447,
      4.5878,
      4.7275,
      4.7,
      4.6284,
      4.7715,
      4.7415,
      4.641,
      4.7657,
      4.7595
    ]
  },
  "training": {
    "iters": [
      0,
      10,
      20,
      30,
      40,
      50,
      60,
      70,
      80,
      90,
      100,
      110,
      120,
      130,
      140,
      150,
      160,
      170,
      180,
      190,
      200,
      210,
      220,
      230,
      240,
      250,
      260,
      270,
      280,
      290,
      300,
      310,
      320,
      330,
      340,
      350,
      360,
      370,
      380,
      390,
      400,
      410,
      420,
      430,
      440,
      450,
      460,
      470,
      480,
      490,
      500,
      510,
      520,
      530,
      540,
      550,
      560,
      570,
      580,
      590,
      600,
      610,
      620,
      630,
      640,
      650,
      660,
      670,
      680,
      690,
      700,
      710,
      720,
      730,
      740,
      750,
      760,
      770,
      780,
      790,
      800,
      810,
      820,
      830,
      840,
      850,
      860,
      870,
      880,
      890,
      900,
      910,
      920,
      930,
      940,
      950,
      960,
      970,
      980,
      990,
      1000
    ],
    "loss": [
      10.8566,
      9.1152,
      8.667,
      7.7726,
      6.9688,
      6.3597,
      5.9985,
      5.7272,
      5.6084,
      5.7208,
      5.4448,
      5.2887,
      5.267,
      5.0374,
      4.7668,
      4.6064,
      4.9806,
      4.501,
      4.6648,
      4.8245,
      4.5771,
      4.4936,
      4.6023,
      4.3126,
      4.4962,
      4.0727,
      4.4034,
      4.088,
      4.1174,
      4.4611,
      4.1301,
      4.4566,
      3.9632,
      3.625,
      4.105,
      3.6384,
      3.8827,
      3.5928,
      3.9707,
      3.7637,
      3.5335,
      3.8509,
      3.4368,
      3.7934,
      3.5537,
      3.7352,
      3.6861,
      3.6469,
      3.6542,
      3.6099,
      3.7958,
      3.3028,
      3.3762,
      3.3504,
      3.4508,
      3.2551,
      3.477,
      3.342,
      3.3573,
      3.3395,
      2.9354,
      3.6801,
      3.0773,
      3.2582,
      3.2125,
      3.4114,
      3.3216,
      3.1313,
      3.2529,
      3.0104,
      2.9951,
      3.3482,
      3.0219,
      3.1979,
      2.9459,
      2.851,
      2.7437,
      2.7916,
      2.859,
      2.9926,
      2.929,
      2.8865,
      2.8462,
      2.752,
      2.7542,
      3.0018,
      2.8391,
      2.7149,
      2.8319,
      2.472,
      2.4786,
      2.7762,
      2.4577,
      2.5763,
      3.0339,
      2.7201,
      2.5877,
      2.531,
      2.7557,
      2.6752,
      2.6688
    ],
    "time": [
      849.46,
      137.73,
      137.0,
      137.21,
      138.98,
      1443.82,
      135.36,
      136.61,
      136.94,
      135.69,
      1935.28,
      136.97,
      135.68,
      136.87,
      139.57,
      1955.42,
      136.47,
      134.26,
      138.75,
      137.49,
      1826.18,
      139.59,
      140.67,
      140.11,
      140.86,
      1863.52,
      154.3,
      139.13,
      139.52,
      138.74,
      1864.11,
      153.12,
      150.87,
      152.15,
      153.04,
      1951.09,
      151.77,
      150.02,
      152.49,
      152.05,
      1912.25,
      151.7,
      151.37,
      152.09,
      149.29,
      1910.5,
      153.02,
      150.89,
      158.6,
      152.73,
      1915.1,
      156.93,
      154.78,
      161.84,
      148.27,
      1940.7,
      138.74,
      139.51,
      152.33,
      157.5,
      1963.08,
      141.54,
      148.32,
      156.26,
      139.35,
      1863.97,
      139.69,
      137.88,
      140.9,
      141.62,
      1936.75,
      139.15,
      138.92,
      138.59,
      136.78,
      1885.32,
      151.07,
      140.12,
      142.77,
      139.22,
      1912.42,
      141.67,
      139.27,
      138.92,
      140.14,
      2058.59,
      135.26,
      133.03,
      137.35,
      131.53,
      1845.42,
      133.31,
      130.23,
      125.61,
      126.26,
      1812.97,
      127.16,
      136.47,
      137.79,
      140.54,
      1927.38
    ],
    "mfu": [
      0,
      14.68,
      14.68,
      14.69,
      14.67,
      13.35,
      13.51,
      13.63,
      13.75,
      13.86,
      12.58,
      12.8,
      13.01,
      13.18,
      13.31,
      12.09,
      12.36,
      12.63,
      12.82,
      13.01,
      11.82,
      12.09,
      12.31,
      12.53,
      12.71,
      11.55,
      11.7,
      11.98,
      12.23,
      12.47,
      11.33,
      11.52,
      11.7,
      11.86,
      12.0,
      10.9,
      11.14,
      11.38,
      11.56,
      11.74,
      10.67,
      10.93,
      11.18,
      11.39,
      11.6,
      10.55,
      10.81,
      11.07,
      11.24,
      11.44,
      10.4,
      10.65,
      10.89,
      11.05,
      11.31,
      10.28,
      10.71,
      11.09,
      11.31,
      11.46,
      10.42,
      10.8,
      11.09,
      11.27,
      11.59,
      10.54,
      10.94,
      11.31,
      11.61,
      11.88,
      10.79,
      11.17,
      11.51,
      11.81,
      12.11,
      11.01,
      11.24,
      11.56,
      11.82,
      12.09,
      10.99,
      11.32,
      11.64,
      11.93,
      12.18,
      11.06,
      11.45,
      11.82,
      12.11,
      12.44,
      11.3,
      11.69,
      12.07,
      12.47,
      12.83,
      11.66,
      12.08,
      12.35,
      12.58,
      12.76,
      11.59
    ]
  },
  "model_info": {
    "parameters": "123.59M",
    "n_layer": "12",
    "n_head": "12",
    "n_embd": "768",
    "block_size": "256",
    "batch_size": "8",
    "learning_rate": 0.0003,
    "dropout": 0.1,
    "max_iters": "1000",
    "dataset": "shakespeare"
  }
}