{
  "validation": {
    "steps": [
      0,
      50,
      100,
      150,
      200,
      250,
      300,
      350,
      400,
      450,
      500,
      550,
      600,
      650,
      700,
      750,
      800,
      850,
      900,
      950,
      1000
    ],
    "train_loss": [
      10.8694,
      6.3049,
      5.5112,
      4.9567,
      4.5129,
      4.3364,
      4.1988,
      4.0563,
      3.9195,
      3.8094,
      3.7974,
      3.6409,
      3.6132,
      3.5044,
      3.3949,
      3.4342,
      3.3118,
      3.2893,
      3.2557,
      3.2851,
      3.1605
    ],
    "val_loss": [
      10.855,
      6.3359,
      5.6902,
      5.2746,
      5.091,
      5.0365,
      4.8751,
      4.8353,
      4.7601,
      4.6722,
      4.6888,
      4.7436,
      4.7534,
      4.8614,
      4.7999,
      4.7321,
      4.8573,
      4.7934,
      4.6948,
      4.8051,
      4.7832
    ]
  },
  "training": {
    "iters": [
      0,
      10,
      20,
      30,
      40,
      50,
      60,
      70,
      80,
      90,
      100,
      110,
      120,
      130,
      140,
      150,
      160,
      170,
      180,
      190,
      200,
      210,
      220,
      230,
      240,
      250,
      260,
      270,
      280,
      290,
      300,
      310,
      320,
      330,
      340,
      350,
      360,
      370,
      380,
      390,
      400,
      410,
      420,
      430,
      440,
      450,
      460,
      470,
      480,
      490,
      500,
      510,
      520,
      530,
      540,
      550,
      560,
      570,
      580,
      590,
      600,
      610,
      620,
      630,
      640,
      650,
      660,
      670,
      680,
      690,
      700,
      710,
      720,
      730,
      740,
      750,
      760,
      770,
      780,
      790,
      800,
      810,
      820,
      830,
      840,
      850,
      860,
      870,
      880,
      890,
      900,
      910,
      920,
      930,
      940,
      950,
      960,
      970,
      980,
      990,
      1000
    ],
    "loss": [
      10.8562,
      9.1148,
      8.6676,
      7.777,
      6.9743,
      6.3606,
      6.0272,
      5.7427,
      5.5858,
      5.8097,
      5.483,
      5.3362,
      5.2953,
      5.0954,
      4.8145,
      4.6569,
      5.0439,
      4.5797,
      4.7457,
      4.8431,
      4.6268,
      4.5734,
      4.6702,
      4.4084,
      4.6007,
      4.205,
      4.5249,
      4.2375,
      4.2967,
      4.5943,
      4.3153,
      4.6155,
      4.1404,
      3.8345,
      4.2663,
      3.9131,
      4.0622,
      3.8029,
      4.2003,
      3.9819,
      3.728,
      4.0773,
      3.7155,
      4.0303,
      3.8458,
      3.9882,
      3.9429,
      3.9396,
      3.9269,
      3.9122,
      4.0739,
      3.581,
      3.7192,
      3.6579,
      3.8217,
      3.6089,
      3.8168,
      3.7392,
      3.7291,
      3.7535,
      3.373,
      4.1134,
      3.4857,
      3.6885,
      3.6725,
      3.8468,
      3.7688,
      3.5888,
      3.7128,
      3.5078,
      3.4707,
      3.7846,
      3.4938,
      3.666,
      3.4915,
      3.4135,
      3.3235,
      3.3574,
      3.4176,
      3.5676,
      3.566,
      3.4584,
      3.4249,
      3.3855,
      3.2921,
      3.6001,
      3.4019,
      3.3229,
      3.4214,
      3.1415,
      3.0881,
      3.3893,
      3.1158,
      3.1917,
      3.5894,
      3.3754,
      3.2056,
      3.203,
      3.3784,
      3.3438,
      3.4052
    ],
    "time": [
      965.27,
      157.44,
      153.07,
      153.29,
      153.7,
      1497.33,
      160.95,
      160.13,
      160.01,
      164.75,
      1980.88,
      148.55,
      162.16,
      152.07,
      161.82,
      1922.73,
      163.3,
      164.25,
      160.91,
      160.77,
      1914.17,
      163.28,
      161.78,
      159.19,
      166.27,
      1904.72,
      165.1,
      164.66,
      164.13,
      147.86,
      1898.43,
      153.83,
      151.1,
      163.44,
      151.25,
      1963.39,
      151.65,
      155.31,
      152.61,
      154.0,
      1958.32,
      153.63,
      150.98,
      154.11,
      162.45,
      1910.65,
      158.61,
      162.29,
      151.09,
      155.18,
      1910.08,
      163.45,
      165.02,
      154.43,
      154.53,
      1920.49,
      166.67,
      163.13,
      162.67,
      146.32,
      1866.85,
      156.16,
      146.8,
      147.63,
      160.74,
      1915.82,
      147.68,
      147.3,
      148.45,
      147.35,
      1862.4,
      150.88,
      160.58,
      160.67,
      164.53,
      1908.19,
      157.9,
      156.33,
      156.79,
      158.82,
      1897.86,
      155.41,
      146.54,
      168.05,
      169.24,
      1960.22,
      168.71,
      167.96,
      175.41,
      148.13,
      1919.17,
      173.06,
      177.0,
      168.73,
      146.46,
      1958.16,
      159.35,
      160.79,
      155.29,
      160.89,
      1966.78
    ],
    "mfu": [
      0,
      12.84,
      12.88,
      12.91,
      12.93,
      11.77,
      11.85,
      11.93,
      12.0,
      12.03,
      10.93,
      11.19,
      11.32,
      11.52,
      11.62,
      10.56,
      10.74,
      10.9,
      11.06,
      11.21,
      10.2,
      10.42,
      10.62,
      10.83,
      10.96,
      9.97,
      10.2,
      10.41,
      10.6,
      10.91,
      9.92,
      10.24,
      10.56,
      10.74,
      11.0,
      10.0,
      10.34,
      10.6,
      10.87,
      11.09,
      10.09,
      10.39,
      10.69,
      10.94,
      11.09,
      10.08,
      10.35,
      10.56,
      10.84,
      11.06,
      10.06,
      10.29,
      10.49,
      10.75,
      10.98,
      9.99,
      10.2,
      10.42,
      10.62,
      10.94,
      9.95,
      10.25,
      10.61,
      10.91,
      11.08,
      10.08,
      10.44,
      10.77,
      11.05,
      11.32,
      10.29,
      10.61,
      10.8,
      10.98,
      11.11,
      10.11,
      10.38,
      10.63,
      10.86,
      11.04,
      10.05,
      10.34,
      10.69,
      10.82,
      10.93,
      9.94,
      10.15,
      10.34,
      10.45,
      10.77,
      9.8,
      9.99,
      10.13,
      10.32,
      10.67,
      9.7,
      10.0,
      10.26,
      10.53,
      10.74,
      9.77
    ]
  },
  "model_info": {
    "parameters": "123.59M",
    "n_layer": "12",
    "n_head": "12",
    "n_embd": "768",
    "block_size": "256",
    "batch_size": "8",
    "learning_rate": 0.0003,
    "dropout": 0.1,
    "max_iters": "1000",
    "dataset": "shakespeare"
  }
}