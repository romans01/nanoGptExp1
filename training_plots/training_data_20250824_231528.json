{
  "validation": {
    "steps": [
      0,
      50,
      100,
      150,
      200,
      250,
      300,
      350,
      400,
      450,
      500,
      550,
      600,
      650,
      700,
      750,
      800,
      850,
      900,
      950,
      1000
    ],
    "train_loss": [
      10.8724,
      6.0433,
      4.8945,
      4.4954,
      4.1536,
      3.9989,
      3.8563,
      3.6819,
      3.5517,
      3.4066,
      3.3573,
      3.1704,
      3.0539,
      2.9201,
      2.7642,
      2.7712,
      2.6149,
      2.5584,
      2.4815,
      2.5065,
      2.3724
    ],
    "val_loss": [
      10.8354,
      6.153,
      5.291,
      5.0503,
      4.9598,
      4.9265,
      4.8157,
      4.7896,
      4.7227,
      4.665,
      4.6891,
      4.767,
      4.8122,
      4.9627,
      4.9362,
      4.8858,
      5.0508,
      5.0078,
      4.9164,
      5.0448,
      5.0441
    ]
  },
  "training": {
    "iters": [
      0,
      10,
      20,
      30,
      40,
      50,
      60,
      70,
      80,
      90,
      100,
      110,
      120,
      130,
      140,
      150,
      160,
      170,
      180,
      190,
      200,
      210,
      220,
      230,
      240,
      250,
      260,
      270,
      280,
      290,
      300,
      310,
      320,
      330,
      340,
      350,
      360,
      370,
      380,
      390,
      400,
      410,
      420,
      430,
      440,
      450,
      460,
      470,
      480,
      490,
      500,
      510,
      520,
      530,
      540,
      550,
      560,
      570,
      580,
      590,
      600,
      610,
      620,
      630,
      640,
      650,
      660,
      670,
      680,
      690,
      700,
      710,
      720,
      730,
      740,
      750,
      760,
      770,
      780,
      790,
      800,
      810,
      820,
      830,
      840,
      850,
      860,
      870,
      880,
      890,
      900,
      910,
      920,
      930,
      940,
      950,
      960,
      970,
      980,
      990,
      1000
    ],
    "loss": [
      10.854,
      9.2034,
      8.43,
      7.6181,
      6.7976,
      6.1252,
      5.5501,
      5.1543,
      5.0874,
      5.1699,
      4.8628,
      4.7899,
      4.8537,
      4.5711,
      4.328,
      4.2747,
      4.5856,
      4.187,
      4.3555,
      4.4735,
      4.317,
      4.2826,
      4.3701,
      4.0877,
      4.2032,
      3.9256,
      4.1845,
      3.8195,
      3.9012,
      4.1626,
      4.0268,
      4.1808,
      3.791,
      3.5068,
      3.9432,
      3.5707,
      3.7157,
      3.5291,
      3.7918,
      3.6156,
      3.4197,
      3.6469,
      3.3534,
      3.5847,
      3.4837,
      3.5954,
      3.5957,
      3.4715,
      3.4784,
      3.5572,
      3.6144,
      3.1834,
      3.274,
      3.2708,
      3.284,
      3.1574,
      3.3027,
      3.1975,
      3.2077,
      3.2538,
      2.8669,
      3.5029,
      2.9541,
      3.0819,
      3.0803,
      3.265,
      3.237,
      3.0979,
      3.0986,
      2.8672,
      2.8937,
      3.1768,
      2.9294,
      3.0852,
      2.876,
      2.8266,
      2.7063,
      2.6681,
      2.8048,
      2.8546,
      2.8233,
      2.8562,
      2.7005,
      2.7561,
      2.6123,
      2.9027,
      2.7487,
      2.6102,
      2.7517,
      2.5003,
      2.4018,
      2.6824,
      2.3832,
      2.4734,
      2.9174,
      2.5987,
      2.4559,
      2.4534,
      2.5679,
      2.5456,
      2.6251
    ],
    "time": [
      934.07,
      158.27,
      160.33,
      160.15,
      158.05,
      1527.69,
      157.78,
      157.43,
      158.87,
      157.57,
      1976.26,
      159.61,
      157.35,
      155.71,
      165.37,
      1927.91,
      157.59,
      159.1,
      172.01,
      159.14,
      1935.39,
      158.94,
      169.8,
      171.62,
      167.08,
      1984.51,
      159.84,
      172.55,
      163.26,
      158.72,
      1967.57,
      166.57,
      167.85,
      170.25,
      158.25,
      1908.14,
      159.8,
      166.67,
      169.88,
      159.78,
      1885.76,
      171.26,
      159.25,
      158.36,
      157.25,
      1927.6,
      158.72,
      161.07,
      173.06,
      158.09,
      1935.18,
      160.42,
      163.89,
      156.97,
      155.72,
      1939.22,
      165.71,
      163.95,
      158.5,
      157.74,
      1927.39,
      143.56,
      149.88,
      158.12,
      151.18,
      1946.69,
      160.2,
      165.9,
      144.06,
      143.99,
      1859.72,
      144.24,
      144.54,
      165.41,
      149.86,
      1859.69,
      144.62,
      148.0,
      144.09,
      166.33,
      1974.27,
      188.21,
      164.55,
      167.49,
      176.72,
      1991.47,
      164.8,
      182.32,
      170.97,
      173.82,
      2022.01,
      168.63,
      171.46,
      173.64,
      170.17,
      1989.24,
      170.35,
      174.96,
      169.28,
      174.38,
      2037.56
    ],
    "mfu": [
      0,
      12.77,
      12.75,
      12.74,
      12.75,
      11.6,
      11.72,
      11.84,
      11.92,
      12.02,
      10.92,
      11.09,
      11.27,
      11.44,
      11.52,
      10.47,
      10.71,
      10.91,
      10.99,
      11.16,
      10.15,
      10.41,
      10.56,
      10.68,
      10.82,
      9.84,
      10.12,
      10.28,
      10.49,
      10.71,
      9.75,
      9.98,
      10.19,
      10.36,
      10.6,
      9.65,
      9.95,
      10.16,
      10.34,
      10.57,
      9.62,
      9.84,
      10.12,
      10.39,
      10.63,
      9.68,
      9.98,
      10.24,
      10.38,
      10.62,
      9.66,
      9.96,
      10.2,
      10.46,
      10.72,
      9.75,
      9.99,
      10.23,
      10.48,
      10.71,
      9.75,
      10.18,
      10.51,
      10.74,
      11.0,
      10.0,
      10.27,
      10.46,
      10.82,
      11.14,
      10.13,
      10.52,
      10.87,
      11.0,
      11.25,
      10.23,
      10.61,
      10.91,
      11.22,
      11.32,
      10.29,
      10.33,
      10.53,
      10.68,
      10.76,
      9.78,
      10.03,
      10.14,
      10.31,
      10.44,
      9.49,
      9.74,
      9.95,
      10.12,
      10.29,
      9.37,
      9.62,
      9.81,
      10.02,
      10.18,
      9.26
    ]
  },
  "model_info": {
    "parameters": "123.59M",
    "n_layer": "12",
    "n_head": "12",
    "n_embd": "768",
    "block_size": "256",
    "batch_size": "8",
    "learning_rate": 0.0003,
    "dropout": 0.1,
    "max_iters": "1000",
    "dataset": "shakespeare"
  }
}