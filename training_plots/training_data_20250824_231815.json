{
  "validation": {
    "steps": [
      0,
      50,
      100,
      150,
      200,
      250,
      300,
      350,
      400,
      450,
      500,
      550,
      600,
      650,
      700,
      750,
      800,
      850,
      900,
      950,
      1000
    ],
    "train_loss": [
      10.924,
      14.1369,
      11.875,
      9.2811,
      9.1914,
      8.9875,
      9.3675,
      9.538,
      9.5617,
      9.0614,
      8.8856,
      8.6447,
      8.5281,
      9.1903,
      17.9948,
      10.5955,
      10.7939,
      10.6308,
      10.4218,
      19.3194,
      19.4665
    ],
    "val_loss": [
      10.921,
      14.1732,
      11.7634,
      9.0129,
      9.0412,
      8.9731,
      9.265,
      9.4474,
      9.6041,
      8.9286,
      8.9881,
      8.7615,
      8.5758,
      9.2545,
      17.7202,
      10.6252,
      10.7392,
      10.6312,
      10.4671,
      19.2176,
      19.3096
    ]
  },
  "training": {
    "iters": [
      0,
      10,
      20,
      30,
      40,
      50,
      60,
      70,
      80,
      90,
      100,
      110,
      120,
      130,
      140,
      150,
      160,
      170,
      180,
      190,
      200,
      210,
      220,
      230,
      240,
      250,
      260,
      270,
      280,
      290,
      300,
      310,
      320,
      330,
      340,
      350,
      360,
      370,
      380,
      390,
      400,
      410,
      420,
      430,
      440,
      450,
      460,
      470,
      480,
      490,
      500,
      510,
      520,
      530,
      540,
      550,
      560,
      570,
      580,
      590,
      600,
      610,
      620,
      630,
      640,
      650,
      660,
      670,
      680,
      690,
      700,
      710,
      720,
      730,
      740,
      750,
      760,
      770,
      780,
      790,
      800,
      810,
      820,
      830,
      840,
      850,
      860,
      870,
      880,
      890,
      900,
      910,
      920,
      930,
      940,
      950,
      960,
      970,
      980,
      990,
      1000
    ],
    "loss": [
      10.9877,
      10.9374,
      10.519,
      10.8454,
      9.4457,
      10.4433,
      10.0206,
      9.5431,
      9.5503,
      10.929,
      10.7731,
      9.1376,
      7.6558,
      8.7821,
      8.118,
      7.0138,
      8.1325,
      7.0093,
      6.8159,
      6.8836,
      6.9322,
      6.8857,
      9.7988,
      7.3982,
      7.6411,
      6.9497,
      6.9641,
      6.8813,
      7.1967,
      7.4895,
      6.8131,
      7.1322,
      6.8473,
      6.683,
      6.8423,
      6.5298,
      6.7058,
      6.5811,
      6.8961,
      6.6609,
      7.1193,
      6.8809,
      7.6299,
      6.9573,
      6.9528,
      6.6428,
      6.5326,
      6.7324,
      6.6544,
      6.6092,
      6.8378,
      6.5675,
      6.5656,
      6.894,
      6.8369,
      6.5992,
      6.9486,
      6.8019,
      6.6183,
      6.827,
      10.2054,
      8.8487,
      7.9115,
      7.0046,
      7.4679,
      6.901,
      6.7736,
      6.6727,
      7.0088,
      7.0592,
      7.2096,
      7.0817,
      8.0373,
      6.7996,
      6.6508,
      6.6352,
      6.7547,
      6.642,
      6.6618,
      6.685,
      6.7296,
      6.7415,
      6.6844,
      6.6532,
      6.6949,
      7.562,
      6.646,
      6.7437,
      6.878,
      6.453,
      6.5159,
      6.6148,
      6.7287,
      6.5918,
      7.0572,
      6.5983,
      6.6465,
      6.749,
      6.8573,
      6.7366,
      6.6759
    ],
    "time": [
      877.75,
      124.03,
      125.15,
      131.63,
      130.49,
      1477.78,
      128.17,
      118.76,
      122.42,
      154.78,
      1864.0,
      121.02,
      120.9,
      120.27,
      121.51,
      1845.78,
      132.81,
      121.03,
      119.33,
      134.75,
      1852.66,
      132.29,
      124.85,
      122.31,
      142.28,
      1845.54,
      123.42,
      123.11,
      122.35,
      121.56,
      1854.06,
      121.34,
      123.73,
      122.88,
      132.38,
      1858.21,
      121.18,
      138.08,
      122.88,
      121.73,
      1839.84,
      137.13,
      130.94,
      138.49,
      133.41,
      1858.83,
      135.65,
      119.65,
      122.61,
      127.8,
      1868.26,
      124.79,
      132.91,
      130.13,
      122.32,
      1909.77,
      121.42,
      135.47,
      130.64,
      121.79,
      1838.96,
      126.66,
      120.75,
      127.95,
      139.55,
      1886.66,
      126.2,
      123.5,
      122.43,
      121.8,
      1866.71,
      139.34,
      132.96,
      123.22,
      121.52,
      1857.43,
      122.85,
      140.89,
      132.76,
      121.81,
      1832.7,
      120.35,
      119.94,
      120.13,
      120.12,
      1834.33,
      120.72,
      128.34,
      126.77,
      121.3,
      1894.61,
      134.73,
      119.68,
      138.41,
      126.21,
      1881.93,
      127.53,
      132.68,
      121.6,
      121.39,
      1839.08
    ],
    "mfu": [
      0,
      16.3,
      16.28,
      16.19,
      16.12,
      14.64,
      14.76,
      14.98,
      15.14,
      14.93,
      13.54,
      13.86,
      14.15,
      14.41,
      14.63,
      13.28,
      13.47,
      13.8,
      14.11,
      14.2,
      12.89,
      13.13,
      13.43,
      13.74,
      13.79,
      12.52,
      12.91,
      13.26,
      13.58,
      13.89,
      12.61,
      13.01,
      13.35,
      13.66,
      13.82,
      12.54,
      12.96,
      13.13,
      13.46,
      13.77,
      12.51,
      12.73,
      13.0,
      13.16,
      13.36,
      12.13,
      12.41,
      12.86,
      13.22,
      13.48,
      12.24,
      12.64,
      12.89,
      13.16,
      13.49,
      12.25,
      12.69,
      12.91,
      13.17,
      13.51,
      12.27,
      12.64,
      13.05,
      13.32,
      13.44,
      12.2,
      12.58,
      12.96,
      13.32,
      13.65,
      12.39,
      12.6,
      12.86,
      13.22,
      13.56,
      12.31,
      12.72,
      12.89,
      13.12,
      13.47,
      12.23,
      12.69,
      13.1,
      13.48,
      13.81,
      12.54,
      12.96,
      13.24,
      13.51,
      13.83,
      12.55,
      12.8,
      13.2,
      13.34,
      13.61,
      12.36,
      12.71,
      12.96,
      13.33,
      13.66,
      12.4
    ]
  },
  "model_info": {
    "parameters": "123.59M",
    "n_layer": "12",
    "n_head": "12",
    "n_embd": "768",
    "block_size": "256",
    "batch_size": "8",
    "learning_rate": 0.0003,
    "dropout": 0.1,
    "max_iters": "1000",
    "dataset": "shakespeare"
  }
}