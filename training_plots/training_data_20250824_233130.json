{
  "validation": {
    "steps": [
      0,
      50,
      100,
      150,
      200,
      250,
      300,
      350,
      400,
      450,
      500,
      550,
      600,
      650,
      700,
      750,
      800,
      850,
      900,
      950,
      1000
    ],
    "train_loss": [
      11.0027,
      6.1409,
      4.922,
      4.4981,
      4.1614,
      4.0133,
      3.8871,
      3.7327,
      3.6065,
      3.4642,
      3.4283,
      3.261,
      3.1833,
      3.0668,
      2.9447,
      2.9577,
      2.8228,
      2.7846,
      2.7261,
      2.751,
      2.6257
    ],
    "val_loss": [
      10.9609,
      6.2345,
      5.3077,
      5.0408,
      4.9573,
      4.9147,
      4.8258,
      4.7975,
      4.7339,
      4.6776,
      4.6964,
      4.7353,
      4.7964,
      4.9113,
      4.8694,
      4.8074,
      4.9485,
      4.8944,
      4.7947,
      4.9166,
      4.8963
    ]
  },
  "training": {
    "iters": [
      0,
      10,
      20,
      30,
      40,
      50,
      60,
      70,
      80,
      90,
      100,
      110,
      120,
      130,
      140,
      150,
      160,
      170,
      180,
      190,
      200,
      210,
      220,
      230,
      240,
      250,
      260,
      270,
      280,
      290,
      300,
      310,
      320,
      330,
      340,
      350,
      360,
      370,
      380,
      390,
      400,
      410,
      420,
      430,
      440,
      450,
      460,
      470,
      480,
      490,
      500,
      510,
      520,
      530,
      540,
      550,
      560,
      570,
      580,
      590,
      600,
      610,
      620,
      630,
      640,
      650,
      660,
      670,
      680,
      690,
      700,
      710,
      720,
      730,
      740,
      750,
      760,
      770,
      780,
      790,
      800,
      810,
      820,
      830,
      840,
      850,
      860,
      870,
      880,
      890,
      900,
      910,
      920,
      930,
      940,
      950,
      960,
      970,
      980,
      990,
      1000
    ],
    "loss": [
      11.0804,
      9.4661,
      8.5472,
      7.7276,
      6.8888,
      6.1712,
      5.7104,
      5.2311,
      5.1134,
      5.2253,
      4.8973,
      4.8037,
      4.8654,
      4.5909,
      4.3361,
      4.2928,
      4.6032,
      4.2138,
      4.3712,
      4.4991,
      4.3282,
      4.2981,
      4.4015,
      4.1208,
      4.2357,
      3.9526,
      4.2263,
      3.8571,
      3.9334,
      4.1973,
      4.0665,
      4.2551,
      3.8698,
      3.5516,
      3.9874,
      3.6623,
      3.7748,
      3.5542,
      3.8883,
      3.6801,
      3.5035,
      3.7325,
      3.4064,
      3.656,
      3.5233,
      3.7156,
      3.6725,
      3.5293,
      3.6328,
      3.6224,
      3.6863,
      3.2545,
      3.3746,
      3.3743,
      3.3904,
      3.2695,
      3.4253,
      3.3128,
      3.3498,
      3.3641,
      2.9864,
      3.6357,
      3.1237,
      3.2225,
      3.2468,
      3.3964,
      3.39,
      3.2666,
      3.2575,
      3.0553,
      3.0911,
      3.3204,
      3.1202,
      3.2404,
      3.0437,
      3.0046,
      2.8817,
      2.895,
      2.9942,
      3.0715,
      3.061,
      3.0949,
      2.9243,
      2.9589,
      2.8377,
      3.1036,
      2.9441,
      2.8305,
      2.9599,
      2.7687,
      2.6373,
      2.8855,
      2.6526,
      2.7089,
      3.1486,
      2.8709,
      2.6768,
      2.6802,
      2.7858,
      2.8251,
      2.8781
    ],
    "time": [
      941.51,
      169.4,
      170.49,
      169.58,
      168.38,
      2085.99,
      170.08,
      168.78,
      169.61,
      168.36,
      2323.64,
      169.09,
      169.28,
      174.53,
      167.56,
      2280.82,
      168.44,
      171.0,
      168.67,
      176.69,
      1942.37,
      159.02,
      157.96,
      158.27,
      158.71,
      1935.75,
      159.77,
      156.96,
      161.13,
      160.95,
      1949.81,
      161.84,
      161.01,
      160.41,
      159.07,
      1932.96,
      167.24,
      167.67,
      168.53,
      167.65,
      1961.58,
      170.32,
      169.69,
      173.29,
      167.54,
      1952.83,
      166.62,
      148.26,
      168.56,
      156.63,
      1916.34,
      167.29,
      149.36,
      165.87,
      153.31,
      1963.17,
      164.93,
      148.79,
      147.95,
      160.85,
      1951.19,
      168.51,
      165.2,
      167.95,
      146.66,
      1978.02,
      174.93,
      169.37,
      169.5,
      169.1,
      1948.02,
      171.14,
      172.32,
      167.44,
      167.07,
      1964.94,
      167.84,
      167.0,
      148.04,
      145.38,
      1884.86,
      167.28,
      168.39,
      172.5,
      170.82,
      1972.92,
      167.41,
      152.1,
      171.55,
      152.64,
      1979.0,
      164.31,
      152.29,
      168.22,
      168.66,
      1982.92,
      168.09,
      168.93,
      169.8,
      171.77,
      1978.22
    ],
    "mfu": [
      0,
      11.93,
      11.92,
      11.92,
      11.93,
      10.84,
      10.94,
      11.04,
      11.13,
      11.22,
      10.18,
      10.36,
      10.52,
      10.63,
      10.77,
      9.78,
      10.0,
      10.18,
      10.36,
      10.47,
      9.53,
      9.85,
      10.14,
      10.4,
      10.64,
      9.68,
      9.98,
      10.27,
      10.49,
      10.7,
      9.73,
      10.01,
      10.26,
      10.5,
      10.72,
      9.75,
      9.98,
      10.19,
      10.37,
      10.54,
      9.59,
      9.82,
      10.03,
      10.19,
      10.38,
      9.44,
      9.71,
      10.1,
      10.29,
      10.55,
      9.6,
      9.85,
      10.22,
      10.42,
      10.69,
      9.73,
      9.98,
      10.34,
      10.67,
      10.86,
      9.88,
      10.09,
      10.31,
      10.48,
      10.81,
      9.83,
      10.0,
      10.2,
      10.37,
      10.53,
      9.58,
      9.8,
      9.99,
      10.2,
      10.39,
      9.46,
      9.71,
      9.95,
      10.32,
      10.68,
      9.72,
      9.96,
      10.16,
      10.32,
      10.47,
      9.52,
      9.78,
      10.13,
      10.3,
      10.59,
      9.63,
      9.9,
      10.24,
      10.42,
      10.57,
      9.62,
      9.86,
      10.07,
      10.25,
      10.4,
      9.47
    ]
  },
  "model_info": {
    "parameters": "123.59M",
    "n_layer": "12",
    "n_head": "12",
    "n_embd": "768",
    "block_size": "256",
    "batch_size": "8",
    "learning_rate": 0.0003,
    "dropout": 0.1,
    "max_iters": "1000",
    "dataset": "shakespeare"
  }
}