{
  "validation": {
    "steps": [
      0,
      50,
      100,
      150,
      200,
      250,
      300,
      350,
      400,
      450,
      500,
      550,
      600,
      650,
      700,
      750,
      800,
      850,
      900,
      950,
      1000
    ],
    "train_loss": [
      10.96,
      7.7698,
      5.6886,
      4.987,
      4.6084,
      4.3984,
      4.2469,
      4.1382,
      4.0079,
      3.853,
      3.8249,
      3.6721,
      3.5868,
      3.4695,
      3.3674,
      3.3928,
      3.2654,
      3.2563,
      3.208,
      3.2444,
      3.1305
    ],
    "val_loss": [
      10.9196,
      7.9141,
      6.0676,
      5.5072,
      5.3981,
      5.2421,
      5.0976,
      5.0441,
      4.9734,
      4.8498,
      4.878,
      4.8909,
      4.8816,
      5.0162,
      4.9457,
      4.8422,
      4.9598,
      4.9051,
      4.7571,
      4.8994,
      4.8687
    ]
  },
  "training": {
    "iters": [
      0,
      10,
      20,
      30,
      40,
      50,
      60,
      70,
      80,
      90,
      100,
      110,
      120,
      130,
      140,
      150,
      160,
      170,
      180,
      190,
      200,
      210,
      220,
      230,
      240,
      250,
      260,
      270,
      280,
      290,
      300,
      310,
      320,
      330,
      340,
      350,
      360,
      370,
      380,
      390,
      400,
      410,
      420,
      430,
      440,
      450,
      460,
      470,
      480,
      490,
      500,
      510,
      520,
      530,
      540,
      550,
      560,
      570,
      580,
      590,
      600,
      610,
      620,
      630,
      640,
      650,
      660,
      670,
      680,
      690,
      700,
      710,
      720,
      730,
      740,
      750,
      760,
      770,
      780,
      790,
      800,
      810,
      820,
      830,
      840,
      850,
      860,
      870,
      880,
      890,
      900,
      910,
      920,
      930,
      940,
      950,
      960,
      970,
      980,
      990,
      1000
    ],
    "loss": [
      11.0861,
      10.5164,
      9.5156,
      8.9474,
      8.4965,
      7.8776,
      7.2184,
      6.3507,
      6.0665,
      6.0614,
      5.6411,
      5.4291,
      5.3589,
      5.2931,
      4.9105,
      4.8942,
      5.227,
      4.5592,
      4.8377,
      4.9086,
      4.7381,
      4.7305,
      4.7828,
      4.4452,
      4.7202,
      4.3352,
      4.7137,
      4.294,
      4.3885,
      4.6966,
      4.4052,
      4.6766,
      4.2034,
      3.937,
      4.4211,
      4.0898,
      4.1399,
      3.9059,
      4.2798,
      4.0556,
      3.8293,
      4.0875,
      3.8117,
      4.1505,
      3.92,
      4.0449,
      4.026,
      3.9943,
      4.0199,
      3.9494,
      4.0929,
      3.6187,
      3.7088,
      3.7406,
      3.8126,
      3.6179,
      3.8317,
      3.6991,
      3.7379,
      3.7508,
      3.3287,
      4.1037,
      3.4841,
      3.6556,
      3.6382,
      3.7871,
      3.7351,
      3.5971,
      3.6733,
      3.461,
      3.4567,
      3.7571,
      3.4958,
      3.6435,
      3.4924,
      3.3906,
      3.2902,
      3.3329,
      3.3728,
      3.4966,
      3.5218,
      3.4511,
      3.4064,
      3.3432,
      3.2394,
      3.5146,
      3.3568,
      3.2654,
      3.3728,
      3.1606,
      3.0825,
      3.3451,
      3.0913,
      3.1509,
      3.5477,
      3.3814,
      3.1278,
      3.1289,
      3.3035,
      3.3156,
      3.3491
    ],
    "time": [
      961.8,
      183.33,
      188.14,
      185.75,
      185.97,
      1994.47,
      185.39,
      187.09,
      184.77,
      186.04,
      2006.25,
      184.59,
      193.03,
      191.31,
      187.83,
      1989.92,
      176.73,
      173.19,
      177.69,
      175.11,
      1975.67,
      175.3,
      178.48,
      178.66,
      175.36,
      1984.56,
      177.69,
      174.78,
      176.98,
      176.88,
      1948.67,
      175.57,
      177.08,
      173.93,
      182.38,
      2039.52,
      173.29,
      174.35,
      162.51,
      169.69,
      1977.11,
      163.79,
      164.58,
      163.36,
      163.09,
      1915.78,
      163.45,
      162.91,
      166.98,
      161.41,
      1959.13,
      169.24,
      160.46,
      160.85,
      165.73,
      1902.58,
      185.48,
      170.16,
      194.89,
      182.38,
      1934.28,
      173.94,
      171.57,
      164.9,
      160.91,
      1962.51,
      163.2,
      162.89,
      164.18,
      168.16,
      1913.38,
      166.65,
      168.59,
      184.56,
      163.27,
      1937.18,
      183.83,
      181.26,
      184.56,
      204.65,
      2051.35,
      184.18,
      185.63,
      189.97,
      183.61,
      1995.44,
      184.95,
      184.5,
      182.81,
      185.19,
      1974.65,
      185.46,
      182.76,
      182.89,
      184.91,
      1978.02,
      183.89,
      187.43,
      185.7,
      185.22,
      2080.6
    ],
    "mfu": [
      0,
      11.03,
      11.0,
      10.99,
      10.97,
      9.98,
      10.07,
      10.14,
      10.22,
      10.29,
      9.36,
      9.52,
      9.61,
      9.71,
      9.81,
      8.93,
      9.18,
      9.43,
      9.63,
      9.82,
      8.94,
      9.2,
      9.41,
      9.6,
      9.79,
      8.92,
      9.16,
      9.4,
      9.6,
      9.79,
      8.91,
      9.17,
      9.4,
      9.62,
      9.77,
      8.89,
      9.17,
      9.41,
      9.71,
      9.93,
      9.04,
      9.37,
      9.66,
      9.93,
      10.18,
      9.27,
      9.58,
      9.86,
      10.08,
      10.33,
      9.4,
      9.65,
      9.95,
      10.21,
      10.41,
      9.47,
      9.62,
      9.84,
      9.9,
      10.01,
      9.12,
      9.37,
      9.61,
      9.87,
      10.14,
      9.23,
      9.55,
      9.83,
      10.08,
      10.27,
      9.35,
      9.63,
      9.87,
      9.97,
      10.22,
      9.3,
      9.47,
      9.64,
      9.77,
      9.78,
      8.9,
      9.11,
      9.29,
      9.42,
      9.58,
      8.72,
      8.94,
      9.14,
      9.34,
      9.49,
      8.65,
      8.87,
      9.09,
      9.29,
      9.45,
      8.61,
      8.85,
      9.04,
      9.22,
      9.39,
      8.55
    ]
  },
  "model_info": {
    "parameters": "123.59M",
    "n_layer": "12",
    "n_head": "12",
    "n_embd": "768",
    "block_size": "256",
    "batch_size": "8",
    "learning_rate": 0.0003,
    "dropout": 0.1,
    "max_iters": "1000",
    "dataset": "shakespeare"
  }
}